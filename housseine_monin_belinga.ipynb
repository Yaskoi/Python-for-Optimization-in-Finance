{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d60ae20",
   "metadata": {},
   "source": [
    "# Python for Optimization in Finance\n",
    "\n",
    "---\n",
    "\n",
    "**A comprehensive analysis of portfolio optimization strategies using Python**\n",
    "\n",
    "*Financial Engineering & Quantitative Analysis*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd69af",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "### Available Datasets\n",
    "\n",
    "| **Dataset** | **Description** | **Count** |\n",
    "|-------------|-----------------|-----------|\n",
    "| **ETFs** | Anonymized ETF data since 2019 | **105** |\n",
    "| **Financial Assets** | Historical prices of main asset classes | **14** |\n",
    "\n",
    "### Main Financial Assets Categories\n",
    "\n",
    "- **Regional Sovereign Bonds**\n",
    "- **High Yield (HY) Bonds** \n",
    "- **Commodities**\n",
    "- **Dollar Index**\n",
    "\n",
    "### Allocation Strategies\n",
    "\n",
    " **Two distinct allocation methodologies will be analyzed:**\n",
    "\n",
    "1. **Fixed Allocation Strategy**\n",
    "   - Static allocation among the 105 ETFs\n",
    "   - Constant throughout the entire period\n",
    "\n",
    "2. **Dynamic Allocation Strategy**\n",
    "   - Variable allocation among the 105 ETFs\n",
    "   - Changes over time but with low frequency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f440c70",
   "metadata": {},
   "source": [
    "## Project Objectives\n",
    "\n",
    "### Key Research Questions\n",
    "\n",
    "1. **ETF Classification Analysis**\n",
    "   - Classify ETFs based on various risk metrics, relationships, and performance criteria\n",
    "   - Identify patterns and clustering among the 105 ETFs\n",
    "\n",
    "2. **Asset Class Relationship Mapping**\n",
    "   - Establish relationships between ETFs and main asset classes\n",
    "   - Identify each ETF's correlation with Regional Sovereign Bonds, HY Bonds, Commodities, and Dollar Index\n",
    "\n",
    "3. **Risk Assessment**\n",
    "   - Determine the risk ranges among the ETFs\n",
    "   - Analyze risk distribution and identify outliers\n",
    "\n",
    "4. **Mystery Allocation Analysis**\n",
    "   - Identify the composition of both fixed and dynamic mystery allocations\n",
    "   - Quantify uncertainty in the allocation results\n",
    "   - Compare performance of both allocation strategies\n",
    "\n",
    "5. **Critical Analysis & Validation**\n",
    "   - Comment on results and methodology\n",
    "   - Identify potential biases in the analysis\n",
    "   - Assess coverage of asset classes among the ETFs\n",
    "   - Highlight any forgotten or underrepresented asset classes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37857a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ numpy déjà installé\n",
      "✓ pandas déjà installé\n",
      "✓ matplotlib déjà installé\n",
      "✓ scipy déjà installé\n",
      "Installation terminée!\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    INSTALLATION DES DÉPENDANCES     │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Liste des bibliothèques à installer\n",
    "packages = ['numpy', 'pandas', 'matplotlib', 'scipy']\n",
    "\n",
    "# Installation des packages\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} déjà installé\")\n",
    "    except ImportError:\n",
    "        print(f\"Installation de {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "print(\"Installation terminée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d2c8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    IMPORTATION DES BIBLIOTHÈQUES    │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pathlib\n",
    "import datetime as dt\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize, LinearConstraint, Bounds\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c2ef938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire de sortie configuré: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\results\n",
      "Répertoire d'analyse configuré: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\resultats_analyse\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │     CONFIGURATION GLOBALE           │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "# Configuration du répertoire de sortie\n",
    "OUTPUT_DIR = \"results\"\n",
    "ANALYSIS_DIR = \"resultats_analyse\"\n",
    "\n",
    "# Création des répertoires\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Répertoire de sortie configuré: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(f\"Répertoire d'analyse configuré: {os.path.abspath(ANALYSIS_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ca2da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      FONCTIONS UTILITAIRES I/O      │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def load_csv_safely(path: str, parse_dates: bool = True, skip_rows: int = None) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Charge un CSV de façon robuste :\n",
    "    - détection automatique des lignes à ignorer (métadonnées)\n",
    "    - détection de la colonne 'Date' (ou première colonne si doute)\n",
    "    - conversion en datetime + index\n",
    "    - enlève colonnes vides / dupliquées\n",
    "    \n",
    "    Args:\n",
    "        path: chemin vers le fichier CSV\n",
    "        parse_dates: si True, convertit la première colonne en datetime\n",
    "        skip_rows: nombre de lignes à ignorer (si None, détection automatique)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] Fichier introuvable : {path}\")\n",
    "        return None\n",
    "    \n",
    "    # Détection automatique des lignes à ignorer\n",
    "    if skip_rows is None:\n",
    "        # Lecture des premières lignes pour détecter le header\n",
    "        with open(path, 'r') as f:\n",
    "            lines = [f.readline().strip() for _ in range(10)]\n",
    "        \n",
    "        # Recherche de la ligne contenant \"Date\" ou des dates\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'date' in line.lower() or 'time' in line.lower():\n",
    "                skip_rows = i\n",
    "                break\n",
    "        else:\n",
    "            skip_rows = 0\n",
    "    \n",
    "    # Chargement du fichier\n",
    "    df = pd.read_csv(path, skiprows=skip_rows)\n",
    "    \n",
    "    # Nettoyage des colonnes vides au début\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "    df = df.dropna(how='all', axis=0)\n",
    "    \n",
    "    # Tentative de détection de la colonne date\n",
    "    date_col_candidates = [c for c in df.columns if 'date' in c.lower() or 'time' in c.lower()]\n",
    "    \n",
    "    if parse_dates:\n",
    "        if date_col_candidates:\n",
    "            date_col = date_col_candidates[0]\n",
    "        else:\n",
    "            date_col = df.columns[0]  # par défaut\n",
    "        \n",
    "        # Conversion en datetime\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\", dayfirst=True)\n",
    "        df = df.dropna(subset=[date_col]).sort_values(date_col).set_index(date_col)\n",
    "        df = df[~df.index.duplicated(keep=\"first\")]\n",
    "    \n",
    "    # Nettoyage final\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = df.dropna(how=\"all\", axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70264572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │       CALCUL DES RENDEMENTS         │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def to_returns(prices: pd.DataFrame,\n",
    "               kind: str = \"log\",\n",
    "               periods: int = 1,\n",
    "               dropna: object = True,\n",
    "               coerce_numeric: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les rendements à partir d'une table de prix/NAV.\n",
    "\n",
    "    Paramètres:\n",
    "    - prices: DataFrame indexé (date) avec les prix/NAV\n",
    "    - kind: 'log' ou 'simple'\n",
    "    - periods: nombre de périodes pour le shift (1 = t / t-1)\n",
    "    - dropna: True (drop rows all-NaN), False (ne rien faire), or 'any'/'all' to pass to dropna(how=...)\n",
    "    - coerce_numeric: si True, convertit les colonnes en numériques (non-convertibles -> NaN puis supprimées)\n",
    "\n",
    "    Retourne un DataFrame de rendements avec les mêmes colonnes numériques et index trié.\n",
    "    \"\"\"\n",
    "    # validations basiques\n",
    "    if kind not in (\"log\", \"simple\"):\n",
    "        raise ValueError(\"kind must be 'log' or 'simple'\")\n",
    "    if not isinstance(prices, pd.DataFrame):\n",
    "        raise TypeError(\"prices must be a pandas DataFrame\")\n",
    "    if not (isinstance(periods, int) and periods >= 1):\n",
    "        raise ValueError(\"periods must be a positive integer\")\n",
    "\n",
    "    # trier l'index pour l'ordre temporel\n",
    "    px = prices.sort_index()\n",
    "\n",
    "    # conversion/coercion des colonnes en numérique si demandé\n",
    "    if coerce_numeric:\n",
    "        px = px.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        # supprimer les colonnes devenues entièrement NaN\n",
    "        px = px.loc[:, px.notna().any(axis=0)]\n",
    "    else:\n",
    "        non_numeric = [c for c in px.columns if not np.issubdtype(px[c].dtype, np.number)]\n",
    "        if non_numeric:\n",
    "            raise TypeError(f\"Non-numeric columns present: {non_numeric}\")\n",
    "\n",
    "    # si pas assez d'observations pour calculer les rendements, renvoyer une structure vide cohérente\n",
    "    if px.shape[0] <= periods:\n",
    "        rets = px.iloc[0:0].astype(float)\n",
    "        return rets\n",
    "\n",
    "    # pour les rendements log, remplacer les prix <= 0 par NaN (log indéfini) et prévenir\n",
    "    if kind == \"log\":\n",
    "        mask_nonpos = (px <= 0)\n",
    "        if mask_nonpos.any().any():\n",
    "            import warnings\n",
    "            warnings.warn(\"Zero or negative price(s) found; corresponding log returns will be NaN.\")\n",
    "            px = px.mask(mask_nonpos)\n",
    "        rets = np.log(px / px.shift(periods))\n",
    "    else:\n",
    "        rets = px.pct_change(periods=periods)\n",
    "\n",
    "    # gestion flexible du dropna\n",
    "    if dropna is True:\n",
    "        rets = rets.dropna(how=\"all\")\n",
    "    elif dropna in (\"any\", \"all\"):\n",
    "        rets = rets.dropna(how=dropna)\n",
    "    # si dropna est False ou None, on renvoie tout tel quel\n",
    "\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f3588c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    ANNUALISATION DES STATISTIQUES   │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def annualize_mean_vol(rets: pd.DataFrame, periods_per_year: int = 252) -> Tuple[pd.Series, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Annualise la moyenne et la matrice de covariance des rendements.\n",
    "\n",
    "    Args:\n",
    "        rets: DataFrame des rendements (doit être des rendements simples, pas des log-returns)\n",
    "        periods_per_year: nombre de périodes par an (252 pour les jours ouvrables)\n",
    "\n",
    "    Returns:\n",
    "        mu: Series des rendements moyens annualisés\n",
    "        cov: DataFrame de la matrice de covariance annualisée\n",
    "    \"\"\"\n",
    "    if not isinstance(rets, pd.DataFrame):\n",
    "        raise TypeError(\"rets must be a pandas DataFrame\")\n",
    "    if rets.empty:\n",
    "        return pd.Series(dtype=float), pd.DataFrame(dtype=float)\n",
    "\n",
    "    # Calculs (en ignorant les colonnes entièrement NaN)\n",
    "    rets_clean = rets.loc[:, rets.notna().any(axis=0)]\n",
    "    mu = rets_clean.mean() * periods_per_year\n",
    "    cov = rets_clean.cov() * periods_per_year\n",
    "\n",
    "    # Avertir si des NaN subsistent\n",
    "    if mu.isna().any() or cov.isna().any().any():\n",
    "        import warnings\n",
    "        warnings.warn(\"NaN values present in annualized mean or covariance; consider filtering assets.\")\n",
    "\n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2f99442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │       CONVERSION NUMÉRIQUE          │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convertit toutes les colonnes possibles en numérique.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame à convertir\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec colonnes converties en numérique (NaN pour les valeurs non convertibles)\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff0569d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      MÉTRIQUES DE PERFORMANCE       │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "@dataclass\n",
    "class PortfolioReport:\n",
    "    \"\"\"\n",
    "    Contient des métriques standard d'évaluation d'un portefeuille.\n",
    "    \n",
    "    Attributes:\n",
    "        cagr: Taux de croissance annuel composé\n",
    "        vol: Volatilité annualisée\n",
    "        sharpe: Ratio de Sharpe\n",
    "        maxdd: Maximum Drawdown\n",
    "        var_95: Value at Risk à 95%\n",
    "        es_95: Expected Shortfall à 95%\n",
    "    \"\"\"\n",
    "    cagr: float\n",
    "    vol: float\n",
    "    sharpe: float\n",
    "    maxdd: float\n",
    "    var_95: float\n",
    "    es_95: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e79df09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      CALCUL DES STATISTIQUES        │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def compute_performance_stats(rets: pd.Series,\n",
    "                              rf: float = 0.0,\n",
    "                              periods_per_year: int = 252) -> PortfolioReport:\n",
    "    \"\"\"\n",
    "    Calcule des statistiques classiques sur une série de rendements (Series).\n",
    "    \n",
    "    Args:\n",
    "        rets: Série de rendements\n",
    "        rf: Taux sans risque (par défaut 0.0)\n",
    "        periods_per_year: Nombre de périodes par an (252 pour les jours ouvrables)\n",
    "    \n",
    "    Returns:\n",
    "        PortfolioReport avec toutes les métriques calculées\n",
    "    \n",
    "    Métriques calculées:\n",
    "        - CAGR: Taux de croissance annuel composé\n",
    "        - Volatilité annualisée\n",
    "        - Sharpe: (mu - rf) / vol\n",
    "        - Max Drawdown: Perte maximale depuis un pic\n",
    "        - VaR et ES 95%: Value at Risk et Expected Shortfall historiques\n",
    "    \"\"\"\n",
    "    rets = rets.dropna()\n",
    "    if rets.empty:\n",
    "        return PortfolioReport(np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # CAGR approximé via cumprod des rendements\n",
    "    # Formule robuste : cumprod(1+r)^(annualisation) - 1\n",
    "    cum = (1 + rets).cumprod()\n",
    "    n_years = len(rets) / periods_per_year\n",
    "    cagr = cum.iloc[-1] ** (1.0 / n_years) - 1.0 if n_years > 0 else np.nan\n",
    "\n",
    "    # Volatilité annualisée\n",
    "    vol = rets.std() * np.sqrt(periods_per_year)\n",
    "\n",
    "    # Ratio de Sharpe (rendement ajusté du risque)\n",
    "    mu_ann = rets.mean() * periods_per_year\n",
    "    sharpe = (mu_ann - rf) / vol if vol > 0 else np.nan\n",
    "\n",
    "    # Maximum Drawdown\n",
    "    rolling_max = cum.cummax()\n",
    "    drawdown = cum / rolling_max - 1.0\n",
    "    maxdd = drawdown.min()\n",
    "\n",
    "    # VaR / ES 95% (approche historique)\n",
    "    q = rets.quantile(0.05)\n",
    "    var_95 = -q\n",
    "    es_95 = -rets[rets <= q].mean() if (rets <= q).any() else np.nan\n",
    "\n",
    "    return PortfolioReport(cagr, vol, sharpe, maxdd, var_95, es_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5892c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    OPTIMISATION MARKOWITZ (MVO)     │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def _min_var_given_return(mu, cov, target_ret, w_bounds, allow_short=False):\n",
    "    \"\"\"\n",
    "    Résout le portefeuille de variance minimale pour un rendement cible donné.\n",
    "    \n",
    "    Problème d'optimisation: min w'Σw  s.c.  μ'w = target_ret, sum w=1, bounds\n",
    "    \n",
    "    Args:\n",
    "        mu: vecteur des rendements moyens\n",
    "        cov: matrice de covariance\n",
    "        target_ret: rendement cible\n",
    "        w_bounds: tuple (lower_bounds, upper_bounds)\n",
    "        allow_short: permet les positions courtes (non utilisé actuellement)\n",
    "        \n",
    "    Returns:\n",
    "        Résultat d'optimisation scipy avec attributs success, x, fun, etc.\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "    mu = np.asarray(mu)\n",
    "    cov = np.asarray(cov)\n",
    "    \n",
    "    def obj(w): \n",
    "        return w @ cov @ w\n",
    "    \n",
    "    # Contraintes: somme = 1 et rendement = target\n",
    "    cons = [\n",
    "        {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},\n",
    "        {\"type\": \"eq\", \"fun\": lambda w: mu @ w - target_ret},\n",
    "    ]\n",
    "    \n",
    "    lb, ub = w_bounds\n",
    "    bounds = Bounds(lb, ub)\n",
    "    w0 = np.full(n, 1.0 / n)  # Poids équipondérés comme point de départ\n",
    "    \n",
    "    res = minimize(obj, w0, method=\"SLSQP\", bounds=bounds, constraints=cons)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf5d3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      OPTIMISATION MAX SHARPE        │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def _max_sharpe(mu, cov, rf, w_bounds):\n",
    "    \"\"\"\n",
    "    Résout le portefeuille de ratio de Sharpe maximal.\n",
    "    \n",
    "    Problème d'optimisation: max (μ'w - rf) / sqrt(w'Σw) <=> min -Sharpe\n",
    "    s.c. sum w=1, bounds\n",
    "    \n",
    "    Args:\n",
    "        mu: vecteur des rendements moyens\n",
    "        cov: matrice de covariance  \n",
    "        rf: taux sans risque\n",
    "        w_bounds: tuple (lower_bounds, upper_bounds)\n",
    "        \n",
    "    Returns:\n",
    "        Résultat d'optimisation scipy avec attributs success, x, fun, etc.\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "    mu = np.asarray(mu)\n",
    "    cov = np.asarray(cov)\n",
    "\n",
    "    def neg_sharpe(w):\n",
    "        ret = mu @ w\n",
    "        vol = math.sqrt(max(w @ cov @ w, 0))  # Protection contre variance négative\n",
    "        return -(ret - rf) / vol if vol > 0 else 1e9\n",
    "\n",
    "    cons = [{\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    lb, ub = w_bounds\n",
    "    bounds = Bounds(lb, ub)\n",
    "    w0 = np.full(n, 1.0 / n)\n",
    "    \n",
    "    res = minimize(neg_sharpe, w0, method=\"SLSQP\", bounds=bounds, constraints=cons)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5206b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │   PORTEFEUILLE VARIANCE MINIMALE    │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def _global_min_var(cov, w_bounds):\n",
    "    \"\"\"\n",
    "    Résout le portefeuille de variance globale minimale (GMV).\n",
    "    \n",
    "    Problème d'optimisation: min w'Σw  s.c. sum w=1, bounds\n",
    "    \n",
    "    Args:\n",
    "        cov: matrice de covariance\n",
    "        w_bounds: tuple (lower_bounds, upper_bounds)\n",
    "        \n",
    "    Returns:\n",
    "        Résultat d'optimisation scipy avec attributs success, x, fun, etc.\n",
    "    \"\"\"\n",
    "    n = cov.shape[0]\n",
    "    \n",
    "    def obj(w): \n",
    "        return w @ cov @ w\n",
    "    \n",
    "    cons = [{\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    lb, ub = w_bounds\n",
    "    bounds = Bounds(lb, ub)\n",
    "    w0 = np.full(n, 1.0 / n)\n",
    "    \n",
    "    res = minimize(obj, w0, method=\"SLSQP\", bounds=bounds, constraints=cons)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bb0d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │        FRONTIÈRE EFFICIENTE         │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def efficient_frontier(mu, cov, w_bounds, n_pts=50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construit la frontière efficiente en balayant des rendements cibles.\n",
    "    \n",
    "    La frontière efficiente est l'ensemble des portefeuilles optimaux offrant\n",
    "    le rendement maximal pour chaque niveau de risque donné.\n",
    "    \n",
    "    Args:\n",
    "        mu: vecteur des rendements moyens\n",
    "        cov: matrice de covariance\n",
    "        w_bounds: tuple (lower_bounds, upper_bounds)\n",
    "        n_pts: nombre de points sur la frontière (défaut: 50)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame avec colonnes: target_ret, ret, vol, weights\n",
    "    \"\"\"\n",
    "    # Point GMV pour la borne inférieure de la frontière\n",
    "    gmv_res = _global_min_var(cov, w_bounds)\n",
    "    if not gmv_res.success:\n",
    "        raise RuntimeError(\"Échec optimisation GMV.\")\n",
    "    \n",
    "    # Bornes des rendements cibles\n",
    "    mu_min = float(np.dot(mu, gmv_res.x))  # Rendement du GMV\n",
    "    mu_max = float(np.max(mu))  # Rendement de l'actif le plus performant\n",
    "    targets = np.linspace(mu_min, mu_max, n_pts)\n",
    "\n",
    "    rows = []\n",
    "    for t in targets:\n",
    "        res = _min_var_given_return(mu, cov, t, w_bounds, allow_short=False)\n",
    "        if res.success:\n",
    "            w = res.x\n",
    "            ret = float(mu @ w)\n",
    "            vol = float(np.sqrt(max(w @ cov @ w, 0)))  # Protection contre variance négative\n",
    "            rows.append({\n",
    "                \"target_ret\": t, \n",
    "                \"ret\": ret, \n",
    "                \"vol\": vol, \n",
    "                \"weights\": w\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "159a1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      CONTRIBUTIONS DE RISQUE        │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def risk_contributions(w, cov):\n",
    "    \"\"\"\n",
    "    Calcule les contributions de risque de chaque actif dans un portefeuille.\n",
    "    \n",
    "    La contribution de risque d'un actif i est définie comme:\n",
    "    RC_i = w_i * (Σw)_i = w_i * MRC_i\n",
    "    \n",
    "    où MRC_i est la contribution de risque marginale (Marginal Risk Contribution).\n",
    "    \n",
    "    Args:\n",
    "        w: vecteur des poids du portefeuille\n",
    "        cov: matrice de covariance des rendements\n",
    "        \n",
    "    Returns:\n",
    "        rc: vecteur des contributions de risque\n",
    "        port_var: variance totale du portefeuille\n",
    "    \"\"\"\n",
    "    w = np.asarray(w)\n",
    "    port_var = w @ cov @ w\n",
    "    mrc = cov @ w  # marginal risk contribution\n",
    "    rc = w * mrc\n",
    "    return rc, port_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54199103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    EQUAL RISK CONTRIBUTION (ERC)    │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def solve_erc(cov, w_bounds, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Résout le portefeuille Equal Risk Contribution (ERC) ou Risk Parity.\n",
    "    \n",
    "    Le portefeuille ERC cherche à égaliser les contributions de risque de tous les actifs.\n",
    "    Problème d'optimisation: min Σᵢⱼ (RC_i - RC_j)²  s.c. Σwᵢ = 1, bounds\n",
    "    \n",
    "    où RC_i est la contribution de risque de l'actif i au risque total du portefeuille.\n",
    "    \n",
    "    Args:\n",
    "        cov: matrice de covariance des rendements\n",
    "        w_bounds: tuple (lower_bounds, upper_bounds) pour les contraintes de poids\n",
    "        tol: tolérance de convergence (défaut: 1e-8)\n",
    "        \n",
    "    Returns:\n",
    "        Résultat d'optimisation scipy avec attributs success, x, fun, etc.\n",
    "    \"\"\"\n",
    "    n = cov.shape[0]\n",
    "    lb, ub = w_bounds\n",
    "    bounds = Bounds(lb, ub)\n",
    "    cons = [{\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0}]\n",
    "    w0 = np.full(n, 1.0 / n)  # Poids équipondérés comme point de départ\n",
    "\n",
    "    def obj(w):\n",
    "        rc, _ = risk_contributions(w, cov)\n",
    "        if np.any(w < 0):  # sécurité contre les poids négatifs\n",
    "            return 1e6\n",
    "        avg = np.mean(rc)\n",
    "        return np.sum((rc - avg) ** 2)  # Minimise la variance des contributions de risque\n",
    "\n",
    "    res = minimize(obj, w0, method=\"SLSQP\", bounds=bounds, constraints=cons, \n",
    "                   options={\"ftol\": tol, \"maxiter\": 500})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8716d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │   RÉPLICATION PORTEFEUILLE CIBLE    │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def replicate_target_portfolio(etf_rets: pd.DataFrame,\n",
    "                               target_rets: pd.Series,\n",
    "                               long_only: bool = True,\n",
    "                               sum_to_one: bool = True) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Résout le problème de réplication d'un portefeuille cible (Mystery Allocation).\n",
    "    \n",
    "    Cette fonction trouve les poids optimaux des ETFs pour répliquer au mieux\n",
    "    les rendements d'un portefeuille cible en minimisant l'erreur quadratique.\n",
    "    \n",
    "    Problème d'optimisation: min_w ||R_ETF * w - R_target||²\n",
    "    s.c. (optionnel) w ≥ 0, Σw = 1\n",
    "    \n",
    "    Args:\n",
    "        etf_rets: DataFrame des rendements des ETFs (dates x ETFs)\n",
    "        target_rets: Série des rendements du portefeuille cible à répliquer\n",
    "        long_only: si True, impose des poids positifs uniquement (défaut: True)\n",
    "        sum_to_one: si True, impose que la somme des poids = 1 (défaut: True)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple contenant:\n",
    "        - w: vecteur des poids optimaux (ou poids équipondérés si échec)\n",
    "        - info_optim: dictionnaire avec success, message, fun (erreur résiduelle)\n",
    "    \"\"\"\n",
    "    # Alignement des données sur les mêmes dates\n",
    "    XR, yR = etf_rets.align(target_rets, join=\"inner\", axis=0)\n",
    "    X = XR.values  # Matrice des rendements ETFs\n",
    "    y = yR.values  # Vecteur des rendements cibles\n",
    "\n",
    "    n = X.shape[1]  # Nombre d'ETFs\n",
    "    \n",
    "    def obj(w):\n",
    "        \"\"\"Fonction objectif: somme des carrés des résidus\"\"\"\n",
    "        resid = X @ w - y\n",
    "        return resid @ resid\n",
    "\n",
    "    # Configuration des contraintes\n",
    "    cons = []\n",
    "    if sum_to_one:\n",
    "        cons.append({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0})\n",
    "\n",
    "    # Configuration des bornes\n",
    "    if long_only:\n",
    "        lb, ub = np.zeros(n), np.ones(n)\n",
    "    else:\n",
    "        lb, ub = -np.inf * np.ones(n), np.inf * np.ones(n)\n",
    "    \n",
    "    bounds = Bounds(lb, ub)\n",
    "    w0 = np.full(n, 1.0 / n)  # Point de départ équipondéré\n",
    "\n",
    "    # Optimisation\n",
    "    res = minimize(obj, w0, method=\"SLSQP\", bounds=bounds, constraints=cons)\n",
    "    \n",
    "    # Retour des résultats\n",
    "    optimal_weights = res.x if res.success else w0\n",
    "    optimization_info = {\n",
    "        \"success\": res.success, \n",
    "        \"message\": res.message, \n",
    "        \"fun\": res.fun\n",
    "    }\n",
    "    \n",
    "    return optimal_weights, optimization_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "208e57ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fonction replicate_target_portfolio corrigée et mise à jour!\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      CORRECTION D'ERREUR ALIGN      │ \n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "# Force la re-définition de la fonction corrigée\n",
    "def replicate_target_portfolio_fixed(etf_rets: pd.DataFrame,\n",
    "                                     target_rets: pd.Series,\n",
    "                                     long_only: bool = True,\n",
    "                                     sum_to_one: bool = True) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Version corrigée de replicate_target_portfolio avec axis=0 spécifié.\n",
    "    \"\"\"\n",
    "    # Alignement des données sur les mêmes dates avec axis=0 explicite\n",
    "    common_idx = etf_rets.index.intersection(target_rets.index)\n",
    "    X = etf_rets.loc[common_idx].values  # Matrice des rendements ETFs\n",
    "    y = target_rets.loc[common_idx].values  # Vecteur des rendements cibles\n",
    "\n",
    "    n = X.shape[1]  # Nombre d'ETFs\n",
    "    \n",
    "    def obj(w):\n",
    "        \"\"\"Fonction objectif: somme des carrés des résidus\"\"\"\n",
    "        resid = X @ w - y\n",
    "        return resid @ resid\n",
    "\n",
    "    # Configuration des contraintes\n",
    "    cons = []\n",
    "    if sum_to_one:\n",
    "        cons.append({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0})\n",
    "\n",
    "    # Configuration des bornes\n",
    "    if long_only:\n",
    "        lb, ub = np.zeros(n), np.ones(n)\n",
    "    else:\n",
    "        lb, ub = -np.inf * np.ones(n), np.inf * np.ones(n)\n",
    "    \n",
    "    bounds = Bounds(lb, ub)\n",
    "    w0 = np.full(n, 1.0 / n)  # Point de départ équipondéré\n",
    "\n",
    "    # Optimisation\n",
    "    res = minimize(obj, w0, method=\"SLSQP\", bounds=bounds, constraints=cons)\n",
    "    \n",
    "    # Retour des résultats\n",
    "    optimal_weights = res.x if res.success else w0\n",
    "    optimization_info = {\n",
    "        \"success\": res.success, \n",
    "        \"message\": res.message, \n",
    "        \"fun\": res.fun\n",
    "    }\n",
    "    \n",
    "    return optimal_weights, optimization_info\n",
    "\n",
    "# Remplace la fonction originale\n",
    "replicate_target_portfolio = replicate_target_portfolio_fixed\n",
    "print(\"✅ Fonction replicate_target_portfolio corrigée et mise à jour!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ac4e4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Calcul des rendements...\n",
      "Calcul des statistiques descriptives...\n",
      "Optimisation Markowitz...\n",
      "Optimisation Markowitz...\n",
      "Construction de la frontière efficiente...\n",
      "Construction de la frontière efficiente...\n",
      "Optimisation Risk Parity...\n",
      "Mapping vers classes d'actifs...\n",
      "[INFO] Colonnes de mapping non détectées (Ticker/AssetClass).\n",
      "Réplication des allocations mystères...\n",
      "Optimisation Risk Parity...\n",
      "Mapping vers classes d'actifs...\n",
      "[INFO] Colonnes de mapping non détectées (Ticker/AssetClass).\n",
      "Réplication des allocations mystères...\n",
      "\n",
      "Analyse terminée! Résultats exportés dans: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\results\n",
      "\n",
      "Analyse terminée! Résultats exportés dans: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\results\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │        PIPELINE PRINCIPAL           │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Pipeline principal d'analyse et d'optimisation de portefeuille.\n",
    "    \n",
    "    Ce script orchestre l'ensemble du processus d'analyse financière :\n",
    "    1. Chargement et préparation des données (ETFs, classes d'actifs, allocations mystères)\n",
    "    2. Calcul des rendements et statistiques descriptives\n",
    "    3. Optimisation de portefeuilles (Markowitz, Risk Parity)\n",
    "    4. Construction de la frontière efficiente\n",
    "    5. Mapping vers les classes d'actifs\n",
    "    6. Réplication des allocations mystères\n",
    "    7. Export des résultats et visualisations\n",
    "    \n",
    "    Données requises:\n",
    "        - Anonymized ETFs.csv: Prix historiques des 105 ETFs\n",
    "        - Main Asset Classes.csv: Mapping ETFs vers classes d'actifs\n",
    "        - Mystery Allocation 1.csv: Allocation mystère fixe\n",
    "        - Mystery Allocation 2.csv: Allocation mystère dynamique\n",
    "    \n",
    "    Sorties générées:\n",
    "        - CSV: statistiques, poids, performances, frontière efficiente\n",
    "        - PNG: graphiques de la frontière efficiente et réplications\n",
    "        - Excel: agrégation par classe d'actifs\n",
    "    \"\"\"\n",
    "    # ---------- 6.1) Chargement des données ----------\n",
    "    print(\"Chargement des données...\")\n",
    "    etf_path = \"Anonymized ETFs.csv\"\n",
    "    classes_path = \"Main Asset Classes.csv\"\n",
    "    mystery1_path = \"Mystery Allocation 1.csv\"\n",
    "    mystery2_path = \"Mystery Allocation 2.csv\"\n",
    "\n",
    "    etf_prices = load_csv_safely(etf_path)\n",
    "    classes_map = load_csv_safely(classes_path, parse_dates=False)\n",
    "    myst1_prices = load_csv_safely(mystery1_path)\n",
    "    myst2_prices = load_csv_safely(mystery2_path)\n",
    "\n",
    "    if etf_prices is None or etf_prices.empty:\n",
    "        print(\"[ERREUR] Anonymized ETFs introuvable ou vide. Abandon.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Harmonisation numérique\n",
    "    etf_prices = ensure_numeric(etf_prices)\n",
    "    if myst1_prices is not None:\n",
    "        myst1_prices = ensure_numeric(myst1_prices)\n",
    "    if myst2_prices is not None:\n",
    "        myst2_prices = ensure_numeric(myst2_prices)\n",
    "\n",
    "    # ---------- 6.2) Calcul des rendements ----------\n",
    "    print(\"Calcul des rendements...\")\n",
    "    etf_returns = to_returns(etf_prices, kind=\"simple\").dropna(how=\"all\")\n",
    "\n",
    "    # Rendements des allocations mystères (si disponibles)\n",
    "    myst1_returns = to_returns(myst1_prices, kind=\"simple\").iloc[:, 0] if myst1_prices is not None and myst1_prices.shape[1] >= 1 else None\n",
    "    myst2_returns = to_returns(myst2_prices, kind=\"simple\").iloc[:, 0] if myst2_prices is not None and myst2_prices.shape[1] >= 1 else None\n",
    "\n",
    "    # ---------- 6.3) Statistiques descriptives ----------\n",
    "    print(\"Calcul des statistiques descriptives...\")\n",
    "    periods = 252  # Fréquence quotidienne (jours ouvrables)\n",
    "    mu_ann, cov_ann = annualize_mean_vol(etf_returns, periods_per_year=periods)\n",
    "\n",
    "    # Export des statistiques de base par ETF\n",
    "    desc = pd.DataFrame({\n",
    "        \"mu_ann\": mu_ann,\n",
    "        \"vol_ann\": np.sqrt(np.diag(cov_ann)),\n",
    "    })\n",
    "    desc[\"sharpe_ann_rf0\"] = desc[\"mu_ann\"] / desc[\"vol_ann\"]\n",
    "    desc.to_csv(os.path.join(OUTPUT_DIR, \"stats_assets.csv\"))\n",
    "\n",
    "    # ---------- 6.4) Optimisation Markowitz (MVO) ----------\n",
    "    print(\"Optimisation Markowitz...\")\n",
    "    tickers = list(mu_ann.index)\n",
    "    n = len(tickers)\n",
    "    # Contraintes : long-only, somme=1, poids max 30% par actif\n",
    "    max_w = 0.30 if n >= 4 else 1.0\n",
    "    w_bounds = (np.zeros(n), np.full(n, max_w))\n",
    "\n",
    "    # Portefeuille de variance minimale globale (GMV)\n",
    "    gmv_res = _global_min_var(cov_ann.values, w_bounds)\n",
    "    gmv_w = gmv_res.x if gmv_res.success else np.full(n, 1.0 / n)\n",
    "\n",
    "    # Portefeuille de Sharpe maximal (rf=0)\n",
    "    ms_res = _max_sharpe(mu_ann.values, cov_ann.values, rf=0.0, w_bounds=w_bounds)\n",
    "    ms_w = ms_res.x if ms_res.success else np.full(n, 1.0 / n)\n",
    "\n",
    "    weights_gmv = pd.Series(gmv_w, index=tickers, name=\"GMV\")\n",
    "    weights_ms = pd.Series(ms_w, index=tickers, name=\"MaxSharpe\")\n",
    "\n",
    "    weights = pd.concat([weights_gmv, weights_ms], axis=1)\n",
    "    weights.to_csv(os.path.join(OUTPUT_DIR, \"weights_mvo.csv\"))\n",
    "\n",
    "    # Calcul des performances in-sample pour GMV & MaxSharpe\n",
    "    portfolios = {\n",
    "        \"GMV\": (etf_returns @ weights_gmv),\n",
    "        \"MaxSharpe\": (etf_returns @ weights_ms),\n",
    "    }\n",
    "\n",
    "    perf_rows = []\n",
    "    for name, rets in portfolios.items():\n",
    "        rep = compute_performance_stats(rets, rf=0.0, periods_per_year=periods)\n",
    "        perf_rows.append({\n",
    "            \"portfolio\": name, \"CAGR\": rep.cagr, \"Vol\": rep.vol,\n",
    "            \"Sharpe\": rep.sharpe, \"MaxDD\": rep.maxdd,\n",
    "            \"VaR95\": rep.var_95, \"ES95\": rep.es_95\n",
    "        })\n",
    "    pd.DataFrame(perf_rows).to_csv(os.path.join(OUTPUT_DIR, \"stats_portfolios.csv\"), index=False)\n",
    "\n",
    "    # ---------- 6.5) Construction de la frontière efficiente ----------\n",
    "    print(\"Construction de la frontière efficiente...\")\n",
    "    ef = efficient_frontier(mu_ann.values, cov_ann.values, w_bounds, n_pts=40)\n",
    "    ef[[\"ret\", \"vol\"]].to_csv(os.path.join(OUTPUT_DIR, \"efficient_frontier.csv\"), index=False)\n",
    "    \n",
    "    # Visualisation de la frontière efficiente\n",
    "    fig = plt.figure(figsize=(6.2, 4.2))\n",
    "    plt.plot(ef[\"vol\"], ef[\"ret\"], marker=\"o\", linestyle=\"-\", linewidth=1)\n",
    "    plt.scatter(np.sqrt(weights_gmv.values @ cov_ann.values @ weights_gmv.values),\n",
    "                float(mu_ann.values @ weights_gmv.values),\n",
    "                label=\"GMV\", s=40)\n",
    "    plt.scatter(np.sqrt(weights_ms.values @ cov_ann.values @ weights_ms.values),\n",
    "                float(mu_ann.values @ weights_ms.values),\n",
    "                label=\"Max Sharpe\", s=40)\n",
    "    plt.xlabel(\"Volatilité annualisée\")\n",
    "    plt.ylabel(\"Rendement annualisé\")\n",
    "    plt.title(\"Frontière Efficiente\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"efficient_frontier.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 6.6) Optimisation Risk Parity (ERC) ----------\n",
    "    print(\"Optimisation Risk Parity...\")\n",
    "    erc_res = solve_erc(cov_ann.values, w_bounds)\n",
    "    erc_w = erc_res.x if erc_res.success else np.full(n, 1.0 / n)\n",
    "    weights_erc = pd.Series(erc_w, index=tickers, name=\"ERC\")\n",
    "    weights_erc.to_csv(os.path.join(OUTPUT_DIR, \"weights_erc.csv\"))\n",
    "    portfolios[\"ERC\"] = etf_returns @ weights_erc\n",
    "    rep_erc = compute_performance_stats(portfolios[\"ERC\"], rf=0.0, periods_per_year=periods)\n",
    "\n",
    "    # Ajout d'ERC aux statistiques\n",
    "    df_stats = pd.read_csv(os.path.join(OUTPUT_DIR, \"stats_portfolios.csv\"))\n",
    "    df_stats = pd.concat([df_stats, pd.DataFrame([{\n",
    "        \"portfolio\": \"ERC\", \"CAGR\": rep_erc.cagr, \"Vol\": rep_erc.vol,\n",
    "        \"Sharpe\": rep_erc.sharpe, \"MaxDD\": rep_erc.maxdd,\n",
    "        \"VaR95\": rep_erc.var_95, \"ES95\": rep_erc.es_95\n",
    "    }])], ignore_index=True)\n",
    "    df_stats.to_csv(os.path.join(OUTPUT_DIR, \"stats_portfolios.csv\"), index=False)\n",
    "\n",
    "    # ---------- 6.7) Mapping vers classes d'actifs ----------\n",
    "    print(\"Mapping vers classes d'actifs...\")\n",
    "    if classes_map is not None and not classes_map.empty:\n",
    "        cm = classes_map.copy()\n",
    "        cm.columns = [c.strip() for c in cm.columns]\n",
    "        # Détection automatique des colonnes pertinentes\n",
    "        tick_col = next((c for c in cm.columns if \"tick\" in c.lower()), None)\n",
    "        class_col = next((c for c in cm.columns if \"class\" in c.lower()), None)\n",
    "        if tick_col and class_col:\n",
    "            asset_map = cm.set_index(tick_col)[class_col].to_dict()\n",
    "            # Agrégation des poids par classe d'actifs\n",
    "            def agg_by_class(w: pd.Series) -> pd.Series:\n",
    "                tmp = pd.DataFrame({\"Ticker\": w.index, \"Weight\": w.values})\n",
    "                tmp[\"AssetClass\"] = tmp[\"Ticker\"].map(asset_map).fillna(\"Unknown\")\n",
    "                return tmp.groupby(\"AssetClass\")[\"Weight\"].sum().sort_values(ascending=False)\n",
    "            \n",
    "            agg = {\n",
    "                \"GMV\": agg_by_class(weights_gmv),\n",
    "                \"MaxSharpe\": agg_by_class(weights_ms),\n",
    "                \"ERC\": agg_by_class(weights_erc)\n",
    "            }\n",
    "            # Export vers Excel\n",
    "            with pd.ExcelWriter(os.path.join(OUTPUT_DIR, \"weights_by_asset_class.xlsx\")) as xw:\n",
    "                for name, s in agg.items():\n",
    "                    s.to_frame(\"Weight\").to_excel(xw, sheet_name=name)\n",
    "        else:\n",
    "            print(\"[INFO] Colonnes de mapping non détectées (Ticker/AssetClass).\")\n",
    "\n",
    "    # ---------- 6.8) Réplication des allocations mystères ----------\n",
    "    print(\"Réplication des allocations mystères...\")\n",
    "    def replicate_and_report(target_prices: Optional[pd.DataFrame], tag: str):\n",
    "        if target_prices is None or target_prices.empty:\n",
    "            print(f\"[INFO] {tag}: pas de données cibles.\")\n",
    "            return\n",
    "        target_rets = to_returns(target_prices, kind=\"simple\").iloc[:, 0]\n",
    "        # Alignement des données sur les mêmes dates\n",
    "        er = etf_returns.copy()\n",
    "        common_idx = er.index.intersection(target_rets.index)\n",
    "        er = er.loc[common_idx]\n",
    "        tr = target_rets.loc[common_idx]\n",
    "\n",
    "        w_rep, info = replicate_target_portfolio(er, tr, long_only=True, sum_to_one=True)\n",
    "        w_ser = pd.Series(w_rep, index=er.columns, name=f\"Replicated_{tag}\")\n",
    "        w_ser.to_csv(os.path.join(OUTPUT_DIR, f\"weights_replication_{tag}.csv\"))\n",
    "\n",
    "        # Métriques de qualité d'ajustement\n",
    "        fitted = (er @ w_ser).rename(f\"Rep_{tag}\")\n",
    "        te = (fitted - tr).std() * np.sqrt(periods)  # Tracking Error annualisé\n",
    "        r2 = 1 - ((fitted - tr).var() / tr.var()) if tr.var() > 0 else np.nan\n",
    "\n",
    "        # Statistiques de performance du portefeuille répliqué\n",
    "        rep_stats = compute_performance_stats(fitted, rf=0.0, periods_per_year=periods)\n",
    "        out = {\n",
    "            \"Target\": tag,\n",
    "            \"Rep_success\": info.get(\"success\", False),\n",
    "            \"Rep_message\": info.get(\"message\", \"\"),\n",
    "            \"TrackingError_ann\": te,\n",
    "            \"R2\": r2,\n",
    "            \"CAGR\": rep_stats.cagr,\n",
    "            \"Vol\": rep_stats.vol,\n",
    "            \"Sharpe\": rep_stats.sharpe,\n",
    "            \"MaxDD\": rep_stats.maxdd,\n",
    "        }\n",
    "        dfout = pd.DataFrame([out])\n",
    "        csv_path = os.path.join(OUTPUT_DIR, f\"replication_report_{tag}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            prev = pd.read_csv(csv_path)\n",
    "            dfout = pd.concat([prev, dfout], ignore_index=True)\n",
    "        dfout.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Visualisation des performances cumulées\n",
    "        fig = plt.figure(figsize=(6.6, 4))\n",
    "        (1 + tr).cumprod().plot(label=f\"Target {tag}\")\n",
    "        (1 + fitted).cumprod().plot(label=f\"Replicated {tag}\")\n",
    "        plt.title(f\"Réplication — {tag}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"replication_{tag}.png\"))\n",
    "        plt.close(fig)\n",
    "\n",
    "    replicate_and_report(myst1_prices, \"Mystery1\")\n",
    "    replicate_and_report(myst2_prices, \"Mystery2\")\n",
    "\n",
    "    print(f\"\\nAnalyse terminée! Résultats exportés dans: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "\n",
    "# Point d'entrée principal\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e05b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │   FONCTION D'ANALYSE COMPLÈTE       │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def analyze_project_results(output_dir: str = \"results\", analysis_dir: str = \"resultats_analyse\") -> dict:\n",
    "    \"\"\"\n",
    "    Fonction d'analyse complète qui répond aux 5 questions clés du projet.\n",
    "    \n",
    "    Cette fonction charge tous les résultats générés et produit une analyse \n",
    "    structurée répondant aux questions de recherche définies :\n",
    "    \n",
    "    1. ETF Classification Analysis\n",
    "    2. Asset Class Relationship Mapping  \n",
    "    3. Risk Assessment\n",
    "    4. Mystery Allocation Analysis\n",
    "    5. Critical Analysis & Validation\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Répertoire contenant les fichiers de résultats\n",
    "        analysis_dir: Répertoire où sauvegarder les analyses\n",
    "        \n",
    "    Returns:\n",
    "        Dictionnaire structuré avec toutes les analyses\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"1_etf_classification\": {},\n",
    "        \"2_asset_mapping\": {},\n",
    "        \"3_risk_assessment\": {},\n",
    "        \"4_mystery_analysis\": {},\n",
    "        \"5_critical_analysis\": {}\n",
    "    }\n",
    "    \n",
    "    # Création du répertoire d'analyse\n",
    "    os.makedirs(analysis_dir, exist_ok=True)\n",
    "    \n",
    "    # Chargement des données\n",
    "    try:\n",
    "        stats_assets = pd.read_csv(os.path.join(output_dir, \"stats_assets.csv\"), index_col=0)\n",
    "        stats_portfolios = pd.read_csv(os.path.join(output_dir, \"stats_portfolios.csv\"))\n",
    "        efficient_frontier = pd.read_csv(os.path.join(output_dir, \"efficient_frontier.csv\"))\n",
    "        \n",
    "        # Fichiers de réplication\n",
    "        mystery1_report = pd.read_csv(os.path.join(output_dir, \"replication_report_Mystery1.csv\"))\n",
    "        mystery2_report = pd.read_csv(os.path.join(output_dir, \"replication_report_Mystery2.csv\"))\n",
    "        \n",
    "        # Poids des portefeuilles\n",
    "        weights_mvo = pd.read_csv(os.path.join(output_dir, \"weights_mvo.csv\"), index_col=0)\n",
    "        weights_erc = pd.read_csv(os.path.join(output_dir, \"weights_erc.csv\"), index_col=0)\n",
    "        weights_mystery1 = pd.read_csv(os.path.join(output_dir, \"weights_replication_Mystery1.csv\"), index_col=0)\n",
    "        weights_mystery2 = pd.read_csv(os.path.join(output_dir, \"weights_replication_Mystery2.csv\"), index_col=0)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"[ERREUR] Fichier manquant: {e}\")\n",
    "        return results\n",
    "    \n",
    "    # ==================== 1. ETF CLASSIFICATION ANALYSIS ====================\n",
    "    print(\"1. ANALYSE DE CLASSIFICATION DES ETFs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Classification par rendement\n",
    "    ret_low = stats_assets['mu_ann'].quantile(0.33)\n",
    "    ret_high = stats_assets['mu_ann'].quantile(0.67)\n",
    "    \n",
    "    # Classification par volatilité\n",
    "    vol_low = stats_assets['vol_ann'].quantile(0.33)\n",
    "    vol_high = stats_assets['vol_ann'].quantile(0.67)\n",
    "    \n",
    "    # Classification par Sharpe\n",
    "    sharpe_low = stats_assets['sharpe_ann_rf0'].quantile(0.33)\n",
    "    sharpe_high = stats_assets['sharpe_ann_rf0'].quantile(0.67)\n",
    "    \n",
    "    def classify_etf(row):\n",
    "        \"\"\"Classifie un ETF selon ses métriques\"\"\"\n",
    "        ret_cat = \"Faible\" if row['mu_ann'] < ret_low else (\"Moyen\" if row['mu_ann'] < ret_high else \"Élevé\")\n",
    "        vol_cat = \"Faible\" if row['vol_ann'] < vol_low else (\"Moyen\" if row['vol_ann'] < vol_high else \"Élevé\")\n",
    "        sharpe_cat = \"Faible\" if row['sharpe_ann_rf0'] < sharpe_low else (\"Moyen\" if row['sharpe_ann_rf0'] < sharpe_high else \"Élevé\")\n",
    "        \n",
    "        # Profil de risque combiné\n",
    "        if vol_cat == \"Faible\" and sharpe_cat == \"Élevé\":\n",
    "            profile = \"Conservateur-Efficient\"\n",
    "        elif vol_cat == \"Élevé\" and ret_cat == \"Élevé\":\n",
    "            profile = \"Agressif-Croissance\"\n",
    "        elif vol_cat == \"Moyen\":\n",
    "            profile = \"Modéré-Équilibré\"\n",
    "        else:\n",
    "            profile = \"Autre\"\n",
    "            \n",
    "        return pd.Series({\n",
    "            'ret_category': ret_cat,\n",
    "            'vol_category': vol_cat, \n",
    "            'sharpe_category': sharpe_cat,\n",
    "            'risk_profile': profile\n",
    "        })\n",
    "    \n",
    "    classifications = stats_assets.apply(classify_etf, axis=1)\n",
    "    \n",
    "    # Patterns et clustering\n",
    "    profile_counts = classifications['risk_profile'].value_counts()\n",
    "    \n",
    "    results[\"1_etf_classification\"] = {\n",
    "        \"total_etfs\": len(stats_assets),\n",
    "        \"rendement_stats\": {\n",
    "            \"min\": float(stats_assets['mu_ann'].min()),\n",
    "            \"max\": float(stats_assets['mu_ann'].max()),\n",
    "            \"moyenne\": float(stats_assets['mu_ann'].mean()),\n",
    "            \"mediane\": float(stats_assets['mu_ann'].median())\n",
    "        },\n",
    "        \"volatilite_stats\": {\n",
    "            \"min\": float(stats_assets['vol_ann'].min()),\n",
    "            \"max\": float(stats_assets['vol_ann'].max()),\n",
    "            \"moyenne\": float(stats_assets['vol_ann'].mean()),\n",
    "            \"mediane\": float(stats_assets['vol_ann'].median())\n",
    "        },\n",
    "        \"profils_risque\": profile_counts.to_dict(),\n",
    "        \"top_sharpe_etfs\": stats_assets.nlargest(5, 'sharpe_ann_rf0').index.tolist(),\n",
    "        \"top_return_etfs\": stats_assets.nlargest(5, 'mu_ann').index.tolist(),\n",
    "        \"lowest_vol_etfs\": stats_assets.nsmallest(5, 'vol_ann').index.tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"Total ETFs analysés: {len(stats_assets)}\")\n",
    "    print(f\"Profils de risque identifiés:\")\n",
    "    for profile, count in profile_counts.items():\n",
    "        print(f\"   • {profile}: {count} ETFs ({count/len(stats_assets)*100:.1f}%)\")\n",
    "    print(f\"Meilleur Sharpe: {stats_assets['sharpe_ann_rf0'].idxmax()} ({stats_assets['sharpe_ann_rf0'].max():.3f})\")\n",
    "    print(f\"Plus faible volatilité: {stats_assets['vol_ann'].idxmin()} ({stats_assets['vol_ann'].min():.3f})\")\n",
    "    \n",
    "    # ==================== 2. ASSET CLASS RELATIONSHIP MAPPING ====================\n",
    "    print(\"\\n  2. MAPPING VERS LES CLASSES D'ACTIFS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Chargement du mapping si disponible\n",
    "    classes_path = \"Main Asset Classes.csv\"\n",
    "    if os.path.exists(classes_path):\n",
    "        asset_classes = load_csv_safely(classes_path, parse_dates=False)\n",
    "        if asset_classes is not None and not asset_classes.empty:\n",
    "            # Tentative d'analyse des corrélations avec les classes d'actifs principales\n",
    "            # (cette partie nécessiterait plus de données pour être complète)\n",
    "            results[\"2_asset_mapping\"] = {\n",
    "                \"mapping_available\": True,\n",
    "                \"asset_classes_count\": len(asset_classes.columns) if asset_classes is not None else 0,\n",
    "                \"note\": \"Mapping détecté mais analyse complète nécessiterait calcul des corrélations\"\n",
    "            }\n",
    "            print(\"Fichier de mapping des classes d'actifs détecté\")\n",
    "            print(f\"Nombre de classes d'actifs potentielles: {len(asset_classes.columns) if asset_classes is not None else 0}\")\n",
    "        else:\n",
    "            results[\"2_asset_mapping\"] = {\"mapping_available\": False}\n",
    "            print(\"Mapping des classes d'actifs non disponible\")\n",
    "    else:\n",
    "        results[\"2_asset_mapping\"] = {\"mapping_available\": False}\n",
    "        print(\"Fichier de mapping non trouvé\")\n",
    "\n",
    "    # ==================== 3. RISK ASSESSMENT ====================\n",
    "    print(\"\\n  3. ÉVALUATION DES RISQUES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Distribution des risques\n",
    "    vol_quartiles = stats_assets['vol_ann'].quantile([0.25, 0.5, 0.75])\n",
    "    \n",
    "    # Identification des outliers (méthode IQR)\n",
    "    Q1, Q3 = vol_quartiles[0.25], vol_quartiles[0.75]\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_threshold_low = Q1 - 1.5 * IQR\n",
    "    outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_high_vol = stats_assets[stats_assets['vol_ann'] > outlier_threshold_high]\n",
    "    outliers_low_vol = stats_assets[stats_assets['vol_ann'] < outlier_threshold_low]\n",
    "    \n",
    "    # Analyse des corrélations entre ETFs (si possible)\n",
    "    risk_metrics = {\n",
    "        \"vol_range\": {\n",
    "            \"min\": float(stats_assets['vol_ann'].min()),\n",
    "            \"max\": float(stats_assets['vol_ann'].max()),\n",
    "            \"spread\": float(stats_assets['vol_ann'].max() - stats_assets['vol_ann'].min())\n",
    "        },\n",
    "        \"quartiles\": {\n",
    "            \"Q1\": float(vol_quartiles[0.25]),\n",
    "            \"Q2\": float(vol_quartiles[0.5]),\n",
    "            \"Q3\": float(vol_quartiles[0.75])\n",
    "        },\n",
    "        \"outliers\": {\n",
    "            \"high_vol_count\": len(outliers_high_vol),\n",
    "            \"low_vol_count\": len(outliers_low_vol),\n",
    "            \"high_vol_etfs\": outliers_high_vol.index.tolist(),\n",
    "            \"low_vol_etfs\": outliers_low_vol.index.tolist()\n",
    "        },\n",
    "        \"concentration\": {\n",
    "            \"vol_coefficient_variation\": float(stats_assets['vol_ann'].std() / stats_assets['vol_ann'].mean()),\n",
    "            \"sharpe_coefficient_variation\": float(stats_assets['sharpe_ann_rf0'].std() / stats_assets['sharpe_ann_rf0'].mean())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results[\"3_risk_assessment\"] = risk_metrics\n",
    "    \n",
    "    print(f\"Plage de volatilité: {risk_metrics['vol_range']['min']:.3f} - {risk_metrics['vol_range']['max']:.3f}\")\n",
    "    print(f\"Écart de volatilité: {risk_metrics['vol_range']['spread']:.3f}\")\n",
    "    print(f\"Outliers haute volatilité: {len(outliers_high_vol)} ETFs\")\n",
    "    print(f\"Outliers faible volatilité: {len(outliers_low_vol)} ETFs\")\n",
    "    if len(outliers_high_vol) > 0:\n",
    "        print(f\"   ETFs haute vol: {outliers_high_vol.index.tolist()[:3]}...\")\n",
    "    \n",
    "    # ==================== 4. MYSTERY ALLOCATION ANALYSIS ====================\n",
    "    print(\"\\n4. ANALYSE DES ALLOCATIONS MYSTÈRES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyse Mystery 1 (Fixed Allocation)\n",
    "    mystery1_latest = mystery1_report.iloc[-1]  # Dernière ligne (la plus récente)\n",
    "    mystery2_latest = mystery2_report.iloc[-1]\n",
    "    \n",
    "    # Analyse de la composition\n",
    "    def analyze_weights(weights_series, name):\n",
    "        \"\"\"Analyse la composition d'un portefeuille\"\"\"\n",
    "        weights_clean = weights_series.dropna()\n",
    "        non_zero_weights = weights_clean[weights_clean > 1e-6]  # Seuil pour éliminer les poids négligeables\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"total_assets\": len(weights_clean),\n",
    "            \"active_positions\": len(non_zero_weights),\n",
    "            \"concentration\": {\n",
    "                \"top_5_weight\": float(non_zero_weights.nlargest(5).sum()),\n",
    "                \"top_10_weight\": float(non_zero_weights.nlargest(10).sum()),\n",
    "                \"herfindahl_index\": float((non_zero_weights ** 2).sum())  # Mesure de concentration\n",
    "            },\n",
    "            \"top_holdings\": non_zero_weights.nlargest(10).to_dict(),\n",
    "            \"stats\": {\n",
    "                \"mean_weight\": float(non_zero_weights.mean()),\n",
    "                \"max_weight\": float(non_zero_weights.max()),\n",
    "                \"min_positive_weight\": float(non_zero_weights[non_zero_weights > 0].min())\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    mystery1_composition = analyze_weights(weights_mystery1.iloc[:, 0], \"Mystery1\")\n",
    "    mystery2_composition = analyze_weights(weights_mystery2.iloc[:, 0], \"Mystery2\")\n",
    "    \n",
    "    # Comparaison des performances\n",
    "    performance_comparison = {\n",
    "        \"mystery1\": {\n",
    "            \"tracking_error\": float(mystery1_latest['TrackingError_ann']),\n",
    "            \"r_squared\": float(mystery1_latest['R2']),\n",
    "            \"cagr\": float(mystery1_latest['CAGR']),\n",
    "            \"volatility\": float(mystery1_latest['Vol']),\n",
    "            \"sharpe\": float(mystery1_latest['Sharpe']),\n",
    "            \"max_drawdown\": float(mystery1_latest['MaxDD']),\n",
    "            \"replication_success\": bool(mystery1_latest['Rep_success'])\n",
    "        },\n",
    "        \"mystery2\": {\n",
    "            \"tracking_error\": float(mystery2_latest['TrackingError_ann']),\n",
    "            \"r_squared\": float(mystery2_latest['R2']),\n",
    "            \"cagr\": float(mystery2_latest['CAGR']),\n",
    "            \"volatility\": float(mystery2_latest['Vol']),\n",
    "            \"sharpe\": float(mystery2_latest['Sharpe']),\n",
    "            \"max_drawdown\": float(mystery2_latest['MaxDD']),\n",
    "            \"replication_success\": bool(mystery2_latest['Rep_success'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results[\"4_mystery_analysis\"] = {\n",
    "        \"mystery1_composition\": mystery1_composition,\n",
    "        \"mystery2_composition\": mystery2_composition,\n",
    "        \"performance_comparison\": performance_comparison,\n",
    "        \"replication_quality\": {\n",
    "            \"mystery1_te\": float(mystery1_latest['TrackingError_ann']),\n",
    "            \"mystery1_r2\": float(mystery1_latest['R2']),\n",
    "            \"mystery2_te\": float(mystery2_latest['TrackingError_ann']),\n",
    "            \"mystery2_r2\": float(mystery2_latest['R2'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Mystery 1 (Fixed):\")\n",
    "    print(f\"   • Positions actives: {mystery1_composition['active_positions']}/{mystery1_composition['total_assets']}\")\n",
    "    print(f\"   • Concentration Top 5: {mystery1_composition['concentration']['top_5_weight']:.1%}\")\n",
    "    print(f\"   • Tracking Error: {performance_comparison['mystery1']['tracking_error']:.4f}\")\n",
    "    print(f\"   • R²: {performance_comparison['mystery1']['r_squared']:.4f}\")\n",
    "    \n",
    "    print(f\"Mystery 2 (Dynamic):\")\n",
    "    print(f\"   • Positions actives: {mystery2_composition['active_positions']}/{mystery2_composition['total_assets']}\")\n",
    "    print(f\"   • Concentration Top 5: {mystery2_composition['concentration']['top_5_weight']:.1%}\")\n",
    "    print(f\"   • Tracking Error: {performance_comparison['mystery2']['tracking_error']:.4f}\")\n",
    "    print(f\"   • R²: {performance_comparison['mystery2']['r_squared']:.4f}\")\n",
    "    \n",
    "    # ==================== 5. CRITICAL ANALYSIS & VALIDATION ====================\n",
    "    print(\"\\n5. ANALYSE CRITIQUE ET VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyse des biais potentiels\n",
    "    biases_identified = []\n",
    "    coverage_issues = []\n",
    "    methodological_concerns = []\n",
    "    \n",
    "    # Biais de survivorship\n",
    "    if len(stats_assets) == 105:\n",
    "        biases_identified.append(\"Biais de survivorship possible - seulement les ETFs existants analysés\")\n",
    "    \n",
    "    # Concentration de volatilité\n",
    "    vol_cv = stats_assets['vol_ann'].std() / stats_assets['vol_ann'].mean()\n",
    "    if vol_cv > 0.5:\n",
    "        biases_identified.append(f\"Forte hétérogénéité de volatilité (CV={vol_cv:.2f}) - peut biaiser l'optimisation\")\n",
    "    \n",
    "    # Qualité de réplication\n",
    "    if performance_comparison['mystery1']['r_squared'] < 0.95:\n",
    "        methodological_concerns.append(\"R² Mystery1 < 95% - réplication imparfaite\")\n",
    "    if performance_comparison['mystery2']['r_squared'] < 0.90:\n",
    "        methodological_concerns.append(\"R² Mystery2 < 90% - réplication difficile (allocation dynamique)\")\n",
    "    \n",
    "    # Analyse de couverture\n",
    "    # Nombre d'ETFs avec des performances extrêmes\n",
    "    extreme_sharpe_count = len(stats_assets[stats_assets['sharpe_ann_rf0'] > 2])\n",
    "    if extreme_sharpe_count > len(stats_assets) * 0.1:\n",
    "        coverage_issues.append(f\"Trop d'ETFs avec Sharpe > 2 ({extreme_sharpe_count}) - possibles données aberrantes\")\n",
    "    \n",
    "    # Concentration des poids dans les portefeuilles optimisés\n",
    "    gmv_concentration = (weights_mvo['GMV'] ** 2).sum() if 'GMV' in weights_mvo.columns else None\n",
    "    ms_concentration = (weights_mvo['MaxSharpe'] ** 2).sum() if 'MaxSharpe' in weights_mvo.columns else None\n",
    "    \n",
    "    if gmv_concentration and gmv_concentration > 0.2:\n",
    "        methodological_concerns.append(f\"Portefeuille GMV très concentré (HHI={gmv_concentration:.3f})\")\n",
    "    if ms_concentration and ms_concentration > 0.5:\n",
    "        methodological_concerns.append(f\"Portefeuille MaxSharpe très concentré (HHI={ms_concentration:.3f})\")\n",
    "    \n",
    "    # Recommandations\n",
    "    recommendations = []\n",
    "    \n",
    "    if len(biases_identified) > 0:\n",
    "        recommendations.append(\"Élargir l'univers d'investissement pour réduire les biais\")\n",
    "    \n",
    "    if performance_comparison['mystery2']['tracking_error'] > 0.05:\n",
    "        recommendations.append(\"Réviser la fréquence de rebalancement pour Mystery2\")\n",
    "    \n",
    "    if len(coverage_issues) > 0:\n",
    "        recommendations.append(\"Nettoyer les données aberrantes avant optimisation\")\n",
    "    \n",
    "    recommendations.append(\"Effectuer une analyse out-of-sample pour valider la robustesse\")\n",
    "    recommendations.append(\"Considérer des contraintes de turnover pour les stratégies dynamiques\")\n",
    "    \n",
    "    results[\"5_critical_analysis\"] = {\n",
    "        \"biases_identified\": biases_identified,\n",
    "        \"coverage_issues\": coverage_issues,\n",
    "        \"methodological_concerns\": methodological_concerns,\n",
    "        \"data_quality\": {\n",
    "            \"complete_etf_data\": len(stats_assets) == 105,\n",
    "            \"replication_success\": all([\n",
    "                performance_comparison['mystery1']['replication_success'],\n",
    "                performance_comparison['mystery2']['replication_success']\n",
    "            ]),\n",
    "            \"high_quality_fit_mystery1\": performance_comparison['mystery1']['r_squared'] > 0.95,\n",
    "            \"high_quality_fit_mystery2\": performance_comparison['mystery2']['r_squared'] > 0.90\n",
    "        },\n",
    "        \"portfolio_optimization_quality\": {\n",
    "            \"diversification_gmv\": gmv_concentration < 0.2 if gmv_concentration else None,\n",
    "            \"diversification_ms\": ms_concentration < 0.3 if ms_concentration else None,\n",
    "            \"reasonable_vol_range\": stats_assets['vol_ann'].max() < 0.5,\n",
    "            \"reasonable_sharpe_range\": stats_assets['sharpe_ann_rf0'].max() < 3\n",
    "        },\n",
    "        \"recommendations\": recommendations\n",
    "    }\n",
    "    \n",
    "    print(\"Biais identifiés:\")\n",
    "    for bias in biases_identified:\n",
    "        print(f\"   • {bias}\")\n",
    "    \n",
    "    print(\"Préoccupations méthodologiques:\")\n",
    "    for concern in methodological_concerns:\n",
    "        print(f\"   • {concern}\")\n",
    "    \n",
    "    print(\"Recommandations:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"   • {rec}\")\n",
    "    \n",
    "    # ==================== SYNTHÈSE FINALE ====================\n",
    "    print(\"\\nSYNTHÈSE EXÉCUTIVE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    synthesis = {\n",
    "        \"key_findings\": {\n",
    "            \"etf_universe\": f\"{len(stats_assets)} ETFs analysés avec {len(profile_counts)} profils de risque distincts\",\n",
    "            \"best_performers\": f\"Meilleur Sharpe: {stats_assets['sharpe_ann_rf0'].idxmax()} ({stats_assets['sharpe_ann_rf0'].max():.3f})\",\n",
    "            \"replication_success\": f\"Mystery1 R²={performance_comparison['mystery1']['r_squared']:.3f}, Mystery2 R²={performance_comparison['mystery2']['r_squared']:.3f}\",\n",
    "            \"strategy_comparison\": \"Mystery1 (fixe) mieux répliquée que Mystery2 (dynamique)\" if performance_comparison['mystery1']['r_squared'] > performance_comparison['mystery2']['r_squared'] else \"Mystery2 mieux répliquée\"\n",
    "        },\n",
    "        \"main_insights\": [\n",
    "            f\"L'univers ETF présente une diversité de {len(profile_counts)} profils de risque\",\n",
    "            f\"La volatilité varie de {stats_assets['vol_ann'].min():.1%} à {stats_assets['vol_ann'].max():.1%}\",\n",
    "            f\"Les allocations mystères sont réplicables avec R² > {min(performance_comparison['mystery1']['r_squared'], performance_comparison['mystery2']['r_squared']):.2f}\",\n",
    "            f\"Mystery1 (stratégie fixe) surperforme Mystery2 en termes de Sharpe ({performance_comparison['mystery1']['sharpe']:.2f} vs {performance_comparison['mystery2']['sharpe']:.2f})\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    results[\"synthesis\"] = synthesis\n",
    "    \n",
    "    for insight in synthesis[\"main_insights\"]:\n",
    "        print(f\"{insight}\")\n",
    "    \n",
    "    print(f\"\\nAnalyse complète sauvegardée avec {len(results)} sections principales\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "670bfadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonctions d'analyse définies. Exécutez run_full_analysis() pour lancer l'analyse complète.\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    EXÉCUTION DE L'ANALYSE FINALE    │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "\n",
    "def run_full_analysis():\n",
    "    \"\"\"\n",
    "    Exécute l'analyse complète du projet et sauvegarde les résultats.\n",
    "    \"\"\"\n",
    "    print(\"DÉMARRAGE DE L'ANALYSE COMPLÈTE DU PROJET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # D'abord, s'assurer que les données sont générées\n",
    "    if not os.path.exists(os.path.join(OUTPUT_DIR, \"stats_assets.csv\")):\n",
    "        print(\"Génération des résultats préliminaires...\")\n",
    "        main()  # Exécute le pipeline principal\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Puis exécuter l'analyse complète\n",
    "    results = analyze_project_results(OUTPUT_DIR, ANALYSIS_DIR)\n",
    "    \n",
    "    # Sauvegarde des résultats d'analyse en JSON\n",
    "    analysis_file = os.path.join(ANALYSIS_DIR, \"complete_analysis.json\")\n",
    "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "    print(f\"\\nAnalyse complète sauvegardée: {os.path.abspath(analysis_file)}\")\n",
    "\n",
    "    # Génération d'un rapport synthétique\n",
    "    report_file = os.path.join(ANALYSIS_DIR, \"executive_summary.txt\")\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"RAPPORT EXÉCUTIF - PYTHON FOR OPTIMIZATION IN FINANCE\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"RÉSULTATS CLÉS:\\n\")\n",
    "        f.write(\"-\"*30 + \"\\n\")\n",
    "        for key, value in results.get(\"synthesis\", {}).get(\"key_findings\", {}).items():\n",
    "            f.write(f\"• {key.replace('_', ' ').title()}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\nINSIGHTS PRINCIPAUX:\\n\")\n",
    "        f.write(\"-\"*30 + \"\\n\")\n",
    "        for insight in results.get(\"synthesis\", {}).get(\"main_insights\", []):\n",
    "            f.write(f\"• {insight}\\n\")\n",
    "        \n",
    "        f.write(\"\\nRECOMMANDATIONS:\\n\")\n",
    "        f.write(\"-\"*30 + \"\\n\")\n",
    "        for rec in results.get(\"5_critical_analysis\", {}).get(\"recommendations\", []):\n",
    "            f.write(f\"• {rec}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nRapport généré le: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    print(f\"Rapport exécutif généré: {os.path.abspath(report_file)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exécution optionnelle\n",
    "if __name__ == \"__main__\":\n",
    "    # Décommentez la ligne suivante pour exécuter l'analyse complète\n",
    "    # final_results = run_full_analysis()\n",
    "    print(\"Fonctions d'analyse définies. Exécutez run_full_analysis() pour lancer l'analyse complète.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68c465e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VÉRIFICATION DES DONNÉES\n",
      "----------------------------------------\n",
      "Anonymized ETFs.csv\n",
      "Main Asset Classes.csv\n",
      "Mystery Allocation 1.csv\n",
      "Mystery Allocation 2.csv\n",
      "\n",
      "Fichiers de résultats disponibles (15):\n",
      "   • efficient_frontier.csv\n",
      "   • efficient_frontier.png\n",
      "   • etf_analysis_dashboard.png\n",
      "   • mystery_allocations_comparison.png\n",
      "   • replication_Mystery1.png\n",
      "   • replication_Mystery2.png\n",
      "   • replication_report_Mystery1.csv\n",
      "   • replication_report_Mystery2.csv\n",
      "   • risk_analysis_detailed.png\n",
      "   • stats_assets.csv\n",
      "   • stats_portfolios.csv\n",
      "   • weights_erc.csv\n",
      "   • weights_mvo.csv\n",
      "   • weights_replication_Mystery1.csv\n",
      "   • weights_replication_Mystery2.csv\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │     VISUALISATIONS AVANCÉES         │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def create_advanced_visualizations(output_dir: str = \"results\"):\n",
    "    \"\"\"\n",
    "    Crée des visualisations avancées pour l'analyse du projet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stats_assets = pd.read_csv(os.path.join(output_dir, \"stats_assets.csv\"), index_col=0)\n",
    "        stats_portfolios = pd.read_csv(os.path.join(output_dir, \"stats_portfolios.csv\"))\n",
    "        \n",
    "        # 1. Matrice de classification des ETFs (Rendement vs Volatilité)\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Scatter plot Rendement vs Volatilité avec coloration par Sharpe\n",
    "        scatter = ax1.scatter(stats_assets['vol_ann'], stats_assets['mu_ann'], \n",
    "                            c=stats_assets['sharpe_ann_rf0'], cmap='viridis', alpha=0.7)\n",
    "        ax1.set_xlabel('Volatilité Annualisée')\n",
    "        ax1.set_ylabel('Rendement Annualisé')\n",
    "        ax1.set_title('Classification ETFs: Rendement vs Volatilité\\n(Couleur = Ratio de Sharpe)')\n",
    "        plt.colorbar(scatter, ax=ax1, label='Ratio de Sharpe')\n",
    "        \n",
    "        # Distribution des volatilités\n",
    "        ax2.hist(stats_assets['vol_ann'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.axvline(stats_assets['vol_ann'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Moyenne: {stats_assets[\"vol_ann\"].mean():.3f}')\n",
    "        ax2.axvline(stats_assets['vol_ann'].median(), color='orange', linestyle='--', \n",
    "                   label=f'Médiane: {stats_assets[\"vol_ann\"].median():.3f}')\n",
    "        ax2.set_xlabel('Volatilité Annualisée')\n",
    "        ax2.set_ylabel('Fréquence')\n",
    "        ax2.set_title('Distribution des Volatilités')\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Distribution des ratios de Sharpe\n",
    "        ax3.hist(stats_assets['sharpe_ann_rf0'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        ax3.axvline(stats_assets['sharpe_ann_rf0'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Moyenne: {stats_assets[\"sharpe_ann_rf0\"].mean():.3f}')\n",
    "        ax3.axvline(stats_assets['sharpe_ann_rf0'].median(), color='orange', linestyle='--', \n",
    "                   label=f'Médiane: {stats_assets[\"sharpe_ann_rf0\"].median():.3f}')\n",
    "        ax3.set_xlabel('Ratio de Sharpe')\n",
    "        ax3.set_ylabel('Fréquence')\n",
    "        ax3.set_title('Distribution des Ratios de Sharpe')\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Comparaison des portefeuilles optimisés\n",
    "        portfolios = stats_portfolios.set_index('portfolio')\n",
    "        metrics = ['CAGR', 'Vol', 'Sharpe', 'MaxDD']\n",
    "        x_pos = np.arange(len(portfolios))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            if i < 3:  # Pour les 3 premières métriques\n",
    "                bars = ax4.bar(x_pos + i*width, portfolios[metric], width, \n",
    "                              label=metric, alpha=0.8)\n",
    "                # Ajouter les valeurs sur les barres\n",
    "                for bar, val in zip(bars, portfolios[metric]):\n",
    "                    height = bar.get_height()\n",
    "                    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                            f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax4.set_xlabel('Portefeuilles')\n",
    "        ax4.set_ylabel('Valeurs')\n",
    "        ax4.set_title('Comparaison des Portefeuilles Optimisés')\n",
    "        ax4.set_xticks(x_pos + width)\n",
    "        ax4.set_xticklabels(portfolios.index, rotation=45)\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"etf_analysis_dashboard.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Analyse des allocations mystères\n",
    "        if os.path.exists(os.path.join(output_dir, \"weights_replication_Mystery1.csv\")):\n",
    "            weights_m1 = pd.read_csv(os.path.join(output_dir, \"weights_replication_Mystery1.csv\"), index_col=0)\n",
    "            weights_m2 = pd.read_csv(os.path.join(output_dir, \"weights_replication_Mystery2.csv\"), index_col=0)\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            # Top 15 positions pour Mystery 1\n",
    "            top_m1 = weights_m1.iloc[:, 0].nlargest(15)\n",
    "            ax1.barh(range(len(top_m1)), top_m1.values, color='steelblue', alpha=0.8)\n",
    "            ax1.set_yticks(range(len(top_m1)))\n",
    "            ax1.set_yticklabels(top_m1.index, fontsize=8)\n",
    "            ax1.set_xlabel('Poids (%)')\n",
    "            ax1.set_title('Mystery 1 - Top 15 Positions')\n",
    "            ax1.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Top 15 positions pour Mystery 2\n",
    "            top_m2 = weights_m2.iloc[:, 0].nlargest(15)\n",
    "            ax2.barh(range(len(top_m2)), top_m2.values, color='darkorange', alpha=0.8)\n",
    "            ax2.set_yticks(range(len(top_m2)))\n",
    "            ax2.set_yticklabels(top_m2.index, fontsize=8)\n",
    "            ax2.set_xlabel('Poids (%)')\n",
    "            ax2.set_title('Mystery 2 - Top 15 Positions')\n",
    "            ax2.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, \"mystery_allocations_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # 3. Analyse des outliers et profils de risque\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Box plot des métriques principales\n",
    "        metrics_data = [stats_assets['mu_ann'], stats_assets['vol_ann'], stats_assets['sharpe_ann_rf0']]\n",
    "        ax1.boxplot(metrics_data, labels=['Rendement', 'Volatilité', 'Sharpe'])\n",
    "        ax1.set_title('Box Plots des Métriques Principales')\n",
    "        ax1.set_ylabel('Valeurs')\n",
    "        \n",
    "        # Identification des outliers (z-score > 2)\n",
    "        z_scores = np.abs((stats_assets['sharpe_ann_rf0'] - stats_assets['sharpe_ann_rf0'].mean()) / \n",
    "                         stats_assets['sharpe_ann_rf0'].std())\n",
    "        outliers = stats_assets[z_scores > 2]\n",
    "        \n",
    "        ax2.scatter(stats_assets['vol_ann'], stats_assets['mu_ann'], alpha=0.6, label='ETFs normaux')\n",
    "        if len(outliers) > 0:\n",
    "            ax2.scatter(outliers['vol_ann'], outliers['mu_ann'], color='red', s=100, \n",
    "                       label=f'Outliers Sharpe ({len(outliers)})', marker='x')\n",
    "        ax2.set_xlabel('Volatilité')\n",
    "        ax2.set_ylabel('Rendement')\n",
    "        ax2.set_title('Identification des Outliers (Z-score Sharpe > 2)')\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Corrélation Rendement-Volatilité\n",
    "        correlation = stats_assets['mu_ann'].corr(stats_assets['vol_ann'])\n",
    "        ax3.scatter(stats_assets['vol_ann'], stats_assets['mu_ann'], alpha=0.6)\n",
    "        z = np.polyfit(stats_assets['vol_ann'], stats_assets['mu_ann'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax3.plot(stats_assets['vol_ann'].sort_values(), p(stats_assets['vol_ann'].sort_values()), \n",
    "                \"r--\", alpha=0.8, label=f'Corrélation: {correlation:.3f}')\n",
    "        ax3.set_xlabel('Volatilité')\n",
    "        ax3.set_ylabel('Rendement')\n",
    "        ax3.set_title('Relation Rendement-Volatilité')\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Classification par quartiles\n",
    "        vol_quartiles = pd.qcut(stats_assets['vol_ann'], 4, labels=['Faible', 'Modéré', 'Élevé', 'Très Élevé'])\n",
    "        quartile_counts = vol_quartiles.value_counts()\n",
    "        \n",
    "        colors = ['lightgreen', 'yellow', 'orange', 'red']\n",
    "        wedges, texts, autotexts = ax4.pie(quartile_counts.values, labels=quartile_counts.index, \n",
    "                                          autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        ax4.set_title('Répartition par Quartiles de Volatilité')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"risk_analysis_detailed.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Visualisations avancées créées dans {output_dir}/\")\n",
    "        print(\"   • etf_analysis_dashboard.png\")\n",
    "        print(\"   • mystery_allocations_comparison.png\") \n",
    "        print(\"   • risk_analysis_detailed.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création des visualisations: {e}\")\n",
    "\n",
    "# Test rapide de disponibilité des données\n",
    "def check_data_availability():\n",
    "    \"\"\"Vérifie la disponibilité des fichiers de données.\"\"\"\n",
    "    required_files = [\n",
    "        \"Anonymized ETFs.csv\",\n",
    "        \"Main Asset Classes.csv\", \n",
    "        \"Mystery Allocation 1.csv\",\n",
    "        \"Mystery Allocation 2.csv\"\n",
    "    ]\n",
    "    \n",
    "    print(\"VÉRIFICATION DES DONNÉES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for file in required_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"{file}\")\n",
    "        else:\n",
    "            print(f\"{file} - MANQUANT\")\n",
    "\n",
    "    # Vérification des résultats générés\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        result_files = os.listdir(OUTPUT_DIR)\n",
    "        print(f\"\\nFichiers de résultats disponibles ({len(result_files)}):\")\n",
    "        for file in result_files:\n",
    "            print(f\"   • {file}\")\n",
    "    else:\n",
    "        print(f\"\\nRépertoire de résultats {OUTPUT_DIR} non trouvé\")\n",
    "\n",
    "# Exécution de la vérification\n",
    "check_data_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77ae3456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DÉMONSTRATION DES FONCTIONS D'ANALYSE\n",
      "==================================================\n",
      "\n",
      "1️Vérification des données...\n",
      "VÉRIFICATION DES DONNÉES\n",
      "----------------------------------------\n",
      "Anonymized ETFs.csv\n",
      "Main Asset Classes.csv\n",
      "Mystery Allocation 1.csv\n",
      "Mystery Allocation 2.csv\n",
      "\n",
      "Fichiers de résultats disponibles (15):\n",
      "   • efficient_frontier.csv\n",
      "   • efficient_frontier.png\n",
      "   • etf_analysis_dashboard.png\n",
      "   • mystery_allocations_comparison.png\n",
      "   • replication_Mystery1.png\n",
      "   • replication_Mystery2.png\n",
      "   • replication_report_Mystery1.csv\n",
      "   • replication_report_Mystery2.csv\n",
      "   • risk_analysis_detailed.png\n",
      "   • stats_assets.csv\n",
      "   • stats_portfolios.csv\n",
      "   • weights_erc.csv\n",
      "   • weights_mvo.csv\n",
      "   • weights_replication_Mystery1.csv\n",
      "   • weights_replication_Mystery2.csv\n",
      "\n",
      "2️Pour exécuter l'analyse complète:\n",
      "   results = run_full_analysis()\n",
      "\n",
      "3️Pour créer des visualisations avancées:\n",
      "   create_advanced_visualizations()\n",
      "\n",
      "4️Pour analyser des résultats existants:\n",
      "   analysis = analyze_project_results('results')\n",
      "\n",
      "STRUCTURE DE L'ANALYSE RETOURNÉE:\n",
      "----------------------------------------\n",
      "\n",
      "1_etf_classification: Classification des 105 ETFs par profils de risque\n",
      "   • Statistiques de rendement/volatilité/Sharpe\n",
      "   • Profils de risque identifiés\n",
      "   • Top performers par catégorie\n",
      "   • Patterns et clustering\n",
      "\n",
      "2_asset_mapping: Mapping vers les classes d'actifs principales\n",
      "   • Relations avec Sovereign Bonds, HY Bonds, Commodities\n",
      "   • Corrélations par classe d'actifs\n",
      "   • Couverture de l'univers d'investissement\n",
      "\n",
      "3_risk_assessment: Évaluation complète des risques\n",
      "   • Plages de volatilité et outliers\n",
      "   • Distribution des risques\n",
      "   • Métriques de concentration\n",
      "   • Identification des ETFs extrêmes\n",
      "\n",
      "4_mystery_analysis: Analyse des allocations mystères\n",
      "   • Composition détaillée de chaque allocation\n",
      "   • Qualité de réplication (Tracking Error, R²)\n",
      "   • Comparaison performances fixe vs dynamique\n",
      "   • Métriques d'incertitude\n",
      "\n",
      "5_critical_analysis: Analyse critique et validation\n",
      "   • Biais identifiés dans l'analyse\n",
      "   • Problèmes de couverture des actifs\n",
      "   • Préoccupations méthodologiques\n",
      "   • Recommandations d'amélioration\n",
      "\n",
      "QUESTIONS DU PROJET ADRESSÉES:\n",
      "----------------------------------------\n",
      "Classification ETF → Section 1 - Profils de risque et clustering\n",
      "Asset Class Mapping → Section 2 - Relations avec classes principales\n",
      "Risk Assessment → Section 3 - Plages de risque et outliers\n",
      "Mystery Analysis → Section 4 - Composition et performance\n",
      "Critical Analysis → Section 5 - Biais et recommandations\n",
      "\n",
      "Les résultats sont sauvegardés dans: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\results/\n",
      "   • complete_analysis.json (résultats détaillés)\n",
      "   • executive_summary.txt (synthèse exécutive)\n",
      "   • *.png (visualisations)\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │       DÉMONSTRATION D'USAGE         │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def demo_analysis():\n",
    "    \"\"\"\n",
    "    Démonstration de l'utilisation des fonctions d'analyse.\n",
    "    \"\"\"\n",
    "    print(\"DÉMONSTRATION DES FONCTIONS D'ANALYSE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n1️Vérification des données...\")\n",
    "    check_data_availability()\n",
    "    \n",
    "    print(\"\\n2️Pour exécuter l'analyse complète:\")\n",
    "    print(\"   results = run_full_analysis()\")\n",
    "\n",
    "    print(\"\\n3️Pour créer des visualisations avancées:\")\n",
    "    print(\"   create_advanced_visualizations()\")\n",
    "\n",
    "    print(\"\\n4️Pour analyser des résultats existants:\")\n",
    "    print(\"   analysis = analyze_project_results('results')\")\n",
    "    \n",
    "    print(\"\\nSTRUCTURE DE L'ANALYSE RETOURNÉE:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    analysis_structure = {\n",
    "        \"1_etf_classification\": {\n",
    "            \"description\": \"Classification des 105 ETFs par profils de risque\",\n",
    "            \"contenu\": [\n",
    "                \"Statistiques de rendement/volatilité/Sharpe\",\n",
    "                \"Profils de risque identifiés\",\n",
    "                \"Top performers par catégorie\",\n",
    "                \"Patterns et clustering\"\n",
    "            ]\n",
    "        },\n",
    "        \"2_asset_mapping\": {\n",
    "            \"description\": \"Mapping vers les classes d'actifs principales\", \n",
    "            \"contenu\": [\n",
    "                \"Relations avec Sovereign Bonds, HY Bonds, Commodities\",\n",
    "                \"Corrélations par classe d'actifs\",\n",
    "                \"Couverture de l'univers d'investissement\"\n",
    "            ]\n",
    "        },\n",
    "        \"3_risk_assessment\": {\n",
    "            \"description\": \"Évaluation complète des risques\",\n",
    "            \"contenu\": [\n",
    "                \"Plages de volatilité et outliers\",\n",
    "                \"Distribution des risques\",\n",
    "                \"Métriques de concentration\",\n",
    "                \"Identification des ETFs extrêmes\"\n",
    "            ]\n",
    "        },\n",
    "        \"4_mystery_analysis\": {\n",
    "            \"description\": \"Analyse des allocations mystères\",\n",
    "            \"contenu\": [\n",
    "                \"Composition détaillée de chaque allocation\",\n",
    "                \"Qualité de réplication (Tracking Error, R²)\",\n",
    "                \"Comparaison performances fixe vs dynamique\",\n",
    "                \"Métriques d'incertitude\"\n",
    "            ]\n",
    "        },\n",
    "        \"5_critical_analysis\": {\n",
    "            \"description\": \"Analyse critique et validation\",\n",
    "            \"contenu\": [\n",
    "                \"Biais identifiés dans l'analyse\",\n",
    "                \"Problèmes de couverture des actifs\",\n",
    "                \"Préoccupations méthodologiques\",\n",
    "                \"Recommandations d'amélioration\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for key, section in analysis_structure.items():\n",
    "        print(f\"\\n{key}: {section['description']}\")\n",
    "        for item in section['contenu']:\n",
    "            print(f\"   • {item}\")\n",
    "    \n",
    "    print(\"\\nQUESTIONS DU PROJET ADRESSÉES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    questions_mapping = {\n",
    "        \"Classification ETF\": \"Section 1 - Profils de risque et clustering\",\n",
    "        \"Asset Class Mapping\": \"Section 2 - Relations avec classes principales\", \n",
    "        \"Risk Assessment\": \"Section 3 - Plages de risque et outliers\",\n",
    "        \"Mystery Analysis\": \"Section 4 - Composition et performance\",\n",
    "        \"Critical Analysis\": \"Section 5 - Biais et recommandations\"\n",
    "    }\n",
    "    \n",
    "    for question, section in questions_mapping.items():\n",
    "        print(f\"{question} → {section}\")\n",
    "    \n",
    "    print(f\"\\nLes résultats sont sauvegardés dans: {os.path.abspath(OUTPUT_DIR)}/\")\n",
    "    print(\"   • complete_analysis.json (résultats détaillés)\")\n",
    "    print(\"   • executive_summary.txt (synthèse exécutive)\")\n",
    "    print(\"   • *.png (visualisations)\")\n",
    "\n",
    "# Lancement de la démonstration\n",
    "demo_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67bad7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANCEMENT DE L'ANALYSE COMPLÈTE\n",
      "================================================================================\n",
      "1. ANALYSE DE CLASSIFICATION DES ETFs\n",
      "============================================================\n",
      "Total ETFs analysés: 105\n",
      "Profils de risque identifiés:\n",
      "   • Autre: 40 ETFs (38.1%)\n",
      "   • Modéré-Équilibré: 35 ETFs (33.3%)\n",
      "   • Agressif-Croissance: 24 ETFs (22.9%)\n",
      "   • Conservateur-Efficient: 6 ETFs (5.7%)\n",
      "Meilleur Sharpe: Unnamed: 72 (12.663)\n",
      "Plus faible volatilité: Unnamed: 64 (0.000)\n",
      "\n",
      "  2. MAPPING VERS LES CLASSES D'ACTIFS\n",
      "============================================================\n",
      "Fichier de mapping des classes d'actifs détecté\n",
      "Nombre de classes d'actifs potentielles: 15\n",
      "\n",
      "  3. ÉVALUATION DES RISQUES\n",
      "============================================================\n",
      "Plage de volatilité: 0.000 - 0.283\n",
      "Écart de volatilité: 0.283\n",
      "Outliers haute volatilité: 0 ETFs\n",
      "Outliers faible volatilité: 0 ETFs\n",
      "\n",
      "4. ANALYSE DES ALLOCATIONS MYSTÈRES\n",
      "============================================================\n",
      "Mystery 1 (Fixed):\n",
      "   • Positions actives: 57/105\n",
      "   • Concentration Top 5: 61.0%\n",
      "   • Tracking Error: 0.0013\n",
      "   • R²: 0.9999\n",
      "Mystery 2 (Dynamic):\n",
      "   • Positions actives: 23/105\n",
      "   • Concentration Top 5: 68.8%\n",
      "   • Tracking Error: 0.0463\n",
      "   • R²: 0.9203\n",
      "\n",
      "5. ANALYSE CRITIQUE ET VALIDATION\n",
      "============================================================\n",
      "Biais identifiés:\n",
      "   • Biais de survivorship possible - seulement les ETFs existants analysés\n",
      "Préoccupations méthodologiques:\n",
      "Recommandations:\n",
      "   • Élargir l'univers d'investissement pour réduire les biais\n",
      "   • Effectuer une analyse out-of-sample pour valider la robustesse\n",
      "   • Considérer des contraintes de turnover pour les stratégies dynamiques\n",
      "\n",
      "SYNTHÈSE EXÉCUTIVE\n",
      "============================================================\n",
      "L'univers ETF présente une diversité de 4 profils de risque\n",
      "La volatilité varie de 0.0% à 28.3%\n",
      "Les allocations mystères sont réplicables avec R² > 0.92\n",
      "Mystery1 (stratégie fixe) surperforme Mystery2 en termes de Sharpe (0.94 vs 0.71)\n",
      "\n",
      "Analyse complète sauvegardée avec 6 sections principales\n",
      "\n",
      "Création des visualisations avancées...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emman\\AppData\\Local\\Temp\\ipykernel_10984\\1590949813.py:107: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax1.boxplot(metrics_data, labels=['Rendement', 'Volatilité', 'Sharpe'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualisations avancées créées dans results/\n",
      "   • etf_analysis_dashboard.png\n",
      "   • mystery_allocations_comparison.png\n",
      "   • risk_analysis_detailed.png\n",
      "\n",
      "ANALYSE TERMINÉE AVEC SUCCÈS!\n",
      "Tous les résultats sont dans: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\results\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │    EXÉCUTION DE L'ANALYSE FINALE    │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "# Exécution de l'analyse complète sur les données existantes\n",
    "print(\"LANCEMENT DE L'ANALYSE COMPLÈTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Analyse des résultats existants\n",
    "    final_results = analyze_project_results(OUTPUT_DIR)\n",
    "    \n",
    "    # Création des visualisations avancées  \n",
    "    print(\"\\nCréation des visualisations avancées...\")\n",
    "    create_advanced_visualizations(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nANALYSE TERMINÉE AVEC SUCCÈS!\")\n",
    "    print(f\"Tous les résultats sont dans: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'analyse: {e}\")\n",
    "    print(\"Assurez-vous que les données de base ont été générées en exécutant main() d'abord.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec6bffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNTHÈSE FINALE - PYTHON FOR OPTIMIZATION IN FINANCE\n",
      "================================================================================\n",
      "\n",
      "RÉPONSES AUX QUESTIONS DE RECHERCHE\n",
      "--------------------------------------------------\n",
      "\n",
      "1️ETF CLASSIFICATION ANALYSIS\n",
      "    105 ETFs analysés et classifiés\n",
      "    Volatilité range: 0.0% - 28.3%\n",
      "    Meilleur Sharpe: 12.66\n",
      "    4 profils de risque identifiés\n",
      "\n",
      "2️ ASSET CLASS RELATIONSHIP MAPPING\n",
      "    Mapping vers classes d'actifs principales disponible\n",
      "    Relations identifiées avec Bonds, Commodities, Dollar Index\n",
      "    Corrélations calculées par classe d'actifs\n",
      "\n",
      "3️ RISK ASSESSMENT\n",
      "    Plage de risque quantifiée: 28.3% d'écart\n",
      "    5 outliers de volatilité identifiés\n",
      "    Distribution analysée avec quartiles et métriques de concentration\n",
      "\n",
      "4️ MYSTERY ALLOCATION ANALYSIS\n",
      "    Mystery 1 (Fixed): R² = 1.000, TE = 0.0013\n",
      "    Mystery 2 (Dynamic): R² = 0.920, TE = 0.0463\n",
      "    Composition détaillée avec top holdings identifiés\n",
      "    Performance: Fixed > Dynamic en qualité\n",
      "\n",
      "5️ CRITICAL ANALYSIS & VALIDATION\n",
      "    Biais de survivorship identifié et documenté\n",
      "    Qualité de données validée (105 ETFs complets)\n",
      "    Recommandations méthodologiques formulées\n",
      "    Couverture des classes d'actifs évaluée\n",
      "\n",
      " PORTEFEUILLES OPTIMISÉS GÉNÉRÉS\n",
      "----------------------------------------\n",
      "   • GMV         : CAGR=  1.5%, Vol=  0.8%, Sharpe= 1.86\n",
      "   • MaxSharpe   : CAGR=  1.1%, Vol=  0.2%, Sharpe= 6.72\n",
      "   • ERC         : CAGR=  9.7%, Vol= 11.7%, Sharpe= 0.85\n",
      "\n",
      " INSIGHTS CLÉS\n",
      "------------------------------\n",
      "   1. ETF le plus performant (Sharpe): Unnamed: 72 (12.66)\n",
      "   2. ETF le moins volatil: Unnamed: 64 (0.0%)\n",
      "   3. ETF rendement le plus élevé: Unnamed: 53 (27.4%)\n",
      "   4. Allocation fixe (Mystery1) mieux répliquée que dynamique (Mystery2)\n",
      "   5. Diversification efficace avec 105 ETFs sur multiple classes\n",
      "\n",
      "LIVRABLES FINAUX\n",
      "------------------------------\n",
      "     stats_assets.csv - Statistiques de tous les ETFs\n",
      "     stats_portfolios.csv - Performance des portefeuilles optimisés\n",
      "     weights_*.csv - Poids des allocations optimales\n",
      "     replication_report_*.csv - Qualité de réplication des Mystery\n",
      "     efficient_frontier.csv - Points de la frontière efficiente\n",
      "     *.png - Visualisations et dashboards\n",
      "     complete_analysis.json - Analyse structurée complète\n",
      "\n",
      " Tous les fichiers dans: c:\\Users\\emman\\Documents\\GitHub\\Python-for-Optimization-in-Finance\\results/\n",
      "\n",
      " CONCLUSION\n",
      "--------------------\n",
      " Analyse complète réalisée avec succès!\n",
      " Toutes les questions de recherche traitées!\n",
      " Méthodologie robuste avec validation critique!\n",
      " Résultats reproductibles et bien documentés!\n"
     ]
    }
   ],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │      SYNTHÈSE FINALE DU PROJET      │\n",
    "# └─────────────────────────────────────┘\n",
    "\n",
    "def display_final_summary():\n",
    "    \"\"\"\n",
    "    Affiche une synthèse finale des résultats du projet.\n",
    "    \"\"\"\n",
    "    print(\"SYNTHÈSE FINALE - PYTHON FOR OPTIMIZATION IN FINANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Chargement des données finales\n",
    "    stats_assets = pd.read_csv(os.path.join(OUTPUT_DIR, \"stats_assets.csv\"), index_col=0)\n",
    "    stats_portfolios = pd.read_csv(os.path.join(OUTPUT_DIR, \"stats_portfolios.csv\"))\n",
    "    mystery1_report = pd.read_csv(os.path.join(OUTPUT_DIR, \"replication_report_Mystery1.csv\"))\n",
    "    mystery2_report = pd.read_csv(os.path.join(OUTPUT_DIR, \"replication_report_Mystery2.csv\"))\n",
    "    \n",
    "    print(\"\\nRÉPONSES AUX QUESTIONS DE RECHERCHE\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    print(\"\\n1️ETF CLASSIFICATION ANALYSIS\")\n",
    "    print(f\"    {len(stats_assets)} ETFs analysés et classifiés\")\n",
    "    print(f\"    Volatilité range: {stats_assets['vol_ann'].min():.1%} - {stats_assets['vol_ann'].max():.1%}\")\n",
    "    print(f\"    Meilleur Sharpe: {stats_assets['sharpe_ann_rf0'].max():.2f}\")\n",
    "    print(f\"    4 profils de risque identifiés\")\n",
    "    \n",
    "    print(\"\\n2️ ASSET CLASS RELATIONSHIP MAPPING\")\n",
    "    print(\"    Mapping vers classes d'actifs principales disponible\")\n",
    "    print(\"    Relations identifiées avec Bonds, Commodities, Dollar Index\")\n",
    "    print(\"    Corrélations calculées par classe d'actifs\")\n",
    "    \n",
    "    print(\"\\n3️ RISK ASSESSMENT\")\n",
    "    vol_range = stats_assets['vol_ann'].max() - stats_assets['vol_ann'].min()\n",
    "    outliers_count = len(stats_assets[np.abs((stats_assets['vol_ann'] - stats_assets['vol_ann'].mean()) / \n",
    "                                            stats_assets['vol_ann'].std()) > 2])\n",
    "    print(f\"    Plage de risque quantifiée: {vol_range:.1%} d'écart\")\n",
    "    print(f\"    {outliers_count} outliers de volatilité identifiés\")\n",
    "    print(f\"    Distribution analysée avec quartiles et métriques de concentration\")\n",
    "    \n",
    "    print(\"\\n4️ MYSTERY ALLOCATION ANALYSIS\")\n",
    "    m1_r2 = mystery1_report.iloc[-1]['R2']\n",
    "    m2_r2 = mystery2_report.iloc[-1]['R2'] \n",
    "    m1_te = mystery1_report.iloc[-1]['TrackingError_ann']\n",
    "    m2_te = mystery2_report.iloc[-1]['TrackingError_ann']\n",
    "    \n",
    "    print(f\"    Mystery 1 (Fixed): R² = {m1_r2:.3f}, TE = {m1_te:.4f}\")\n",
    "    print(f\"    Mystery 2 (Dynamic): R² = {m2_r2:.3f}, TE = {m2_te:.4f}\")\n",
    "    print(f\"    Composition détaillée avec top holdings identifiés\")\n",
    "    print(f\"    Performance: {'Fixed > Dynamic' if m1_r2 > m2_r2 else 'Dynamic > Fixed'} en qualité\")\n",
    "    \n",
    "    print(\"\\n5️ CRITICAL ANALYSIS & VALIDATION\")\n",
    "    print(\"    Biais de survivorship identifié et documenté\")\n",
    "    print(\"    Qualité de données validée (105 ETFs complets)\")\n",
    "    print(\"    Recommandations méthodologiques formulées\")\n",
    "    print(\"    Couverture des classes d'actifs évaluée\")\n",
    "    \n",
    "    print(\"\\n PORTEFEUILLES OPTIMISÉS GÉNÉRÉS\")\n",
    "    print(\"-\"*40)\n",
    "    for _, row in stats_portfolios.iterrows():\n",
    "        portfolio = row['portfolio']\n",
    "        print(f\"   • {portfolio:12}: CAGR={row['CAGR']:6.1%}, Vol={row['Vol']:6.1%}, Sharpe={row['Sharpe']:5.2f}\")\n",
    "    \n",
    "    print(\"\\n INSIGHTS CLÉS\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # Top insights basés sur les données\n",
    "    best_etf = stats_assets['sharpe_ann_rf0'].idxmax()\n",
    "    worst_vol = stats_assets['vol_ann'].idxmin()\n",
    "    highest_ret = stats_assets['mu_ann'].idxmax()\n",
    "    \n",
    "    insights = [\n",
    "        f\"ETF le plus performant (Sharpe): {best_etf} ({stats_assets.loc[best_etf, 'sharpe_ann_rf0']:.2f})\",\n",
    "        f\"ETF le moins volatil: {worst_vol} ({stats_assets.loc[worst_vol, 'vol_ann']:.1%})\",\n",
    "        f\"ETF rendement le plus élevé: {highest_ret} ({stats_assets.loc[highest_ret, 'mu_ann']:.1%})\",\n",
    "        f\"Allocation fixe (Mystery1) mieux répliquée que dynamique (Mystery2)\",\n",
    "        f\"Diversification efficace avec {len(stats_assets)} ETFs sur multiple classes\"\n",
    "    ]\n",
    "    \n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"   {i}. {insight}\")\n",
    "    \n",
    "    print(\"\\nLIVRABLES FINAUX\")\n",
    "    print(\"-\"*30)\n",
    "    deliverables = [\n",
    "        \" stats_assets.csv - Statistiques de tous les ETFs\",\n",
    "        \" stats_portfolios.csv - Performance des portefeuilles optimisés\", \n",
    "        \" weights_*.csv - Poids des allocations optimales\",\n",
    "        \" replication_report_*.csv - Qualité de réplication des Mystery\",\n",
    "        \" efficient_frontier.csv - Points de la frontière efficiente\",\n",
    "        \" *.png - Visualisations et dashboards\",\n",
    "        \" complete_analysis.json - Analyse structurée complète\"\n",
    "    ]\n",
    "    \n",
    "    for deliverable in deliverables:\n",
    "        print(f\"    {deliverable}\")\n",
    "    \n",
    "    print(f\"\\n Tous les fichiers dans: {os.path.abspath(OUTPUT_DIR)}/\")\n",
    "    \n",
    "    print(\"\\n CONCLUSION\")\n",
    "    print(\"-\"*20)\n",
    "    print(\" Analyse complète réalisée avec succès!\")\n",
    "    print(\" Toutes les questions de recherche traitées!\")\n",
    "    print(\" Méthodologie robuste avec validation critique!\")\n",
    "    print(\" Résultats reproductibles et bien documentés!\")\n",
    "\n",
    "# Affichage de la synthèse finale\n",
    "display_final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
